{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this project, you will build a neural network of your own design to evaluate the MNIST dataset.\n",
    "\n",
    "Some of the benchmark results on MNIST include can be found [on Yann LeCun's page](http://yann.lecun.com/exdb/mnist/) and include:\n",
    "\n",
    "88% [Lecun et al., 1998](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf)\n",
    "95.3% [Lecun et al., 1998](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf)\n",
    "99.65% [Ciresan et al., 2011](http://people.idsia.ch/~juergen/ijcai2011.pdf)\n",
    "\n",
    "MNIST is a great dataset for sanity checking your models, since the accuracy levels achieved by large convolutional neural networks and small linear models are both quite high. This makes it important to be familiar with the data.\n",
    "\n",
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the PATH to include the user installation directory. \n",
    "import os\n",
    "os.environ['PATH'] = f\"{os.environ['PATH']}:/root/.local/bin\"\n",
    "\n",
    "# Restart the Kernel before you move on to the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important: Restart the Kernel before you move on to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: opencv-python-headless==4.5.3.56 in /root/.local/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (4.5.3.56)\n",
      "Requirement already satisfied: matplotlib==3.4.3 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (3.4.3)\n",
      "Requirement already satisfied: numpy==1.21.2 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (1.21.2)\n",
      "Requirement already satisfied: pillow==7.0.0 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (7.0.0)\n",
      "Requirement already satisfied: bokeh==2.1.1 in /root/.local/lib/python3.7/site-packages (from -r requirements.txt (line 5)) (2.1.1)\n",
      "Requirement already satisfied: torch==1.11.0 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 6)) (1.11.0)\n",
      "Requirement already satisfied: torchvision==0.12.0 in /root/.local/lib/python3.7/site-packages (from -r requirements.txt (line 7)) (0.12.0)\n",
      "Requirement already satisfied: tqdm==4.63.0 in /root/.local/lib/python3.7/site-packages (from -r requirements.txt (line 8)) (4.63.0)\n",
      "Requirement already satisfied: ipywidgets==7.7.0 in /root/.local/lib/python3.7/site-packages (from -r requirements.txt (line 9)) (7.7.0)\n",
      "Requirement already satisfied: livelossplot==0.5.4 in /root/.local/lib/python3.7/site-packages (from -r requirements.txt (line 10)) (0.5.4)\n",
      "Requirement already satisfied: pytest==7.1.1 in /root/.local/lib/python3.7/site-packages (from -r requirements.txt (line 11)) (7.1.1)\n",
      "Requirement already satisfied: pandas==1.3.5 in /root/.local/lib/python3.7/site-packages (from -r requirements.txt (line 12)) (1.3.5)\n",
      "Requirement already satisfied: seaborn==0.11.2 in /root/.local/lib/python3.7/site-packages (from -r requirements.txt (line 13)) (0.11.2)\n",
      "Requirement already satisfied: jupyter==1.0.0 in /root/.local/lib/python3.7/site-packages (from -r requirements.txt (line 14)) (1.0.0)\n",
      "Requirement already satisfied: ipykernel==4.10.0 in /root/.local/lib/python3.7/site-packages (from -r requirements.txt (line 15)) (4.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib==3.4.3->-r requirements.txt (line 2)) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib==3.4.3->-r requirements.txt (line 2)) (0.10.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib==3.4.3->-r requirements.txt (line 2)) (2.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib==3.4.3->-r requirements.txt (line 2)) (2.8.1)\n",
      "Requirement already satisfied: Jinja2>=2.7 in /opt/conda/lib/python3.7/site-packages (from bokeh==2.1.1->-r requirements.txt (line 5)) (2.11.1)\n",
      "Requirement already satisfied: PyYAML>=3.10 in /opt/conda/lib/python3.7/site-packages (from bokeh==2.1.1->-r requirements.txt (line 5)) (5.3)\n",
      "Requirement already satisfied: packaging>=16.8 in /opt/conda/lib/python3.7/site-packages (from bokeh==2.1.1->-r requirements.txt (line 5)) (20.1)\n",
      "Requirement already satisfied: tornado>=5.1 in /opt/conda/lib/python3.7/site-packages (from bokeh==2.1.1->-r requirements.txt (line 5)) (5.1.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from bokeh==2.1.1->-r requirements.txt (line 5)) (3.7.4.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision==0.12.0->-r requirements.txt (line 7)) (2.23.0)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets==7.7.0->-r requirements.txt (line 9)) (5.0.4)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /root/.local/lib/python3.7/site-packages (from ipywidgets==7.7.0->-r requirements.txt (line 9)) (3.0.9)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets==7.7.0->-r requirements.txt (line 9)) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.7/site-packages (from ipywidgets==7.7.0->-r requirements.txt (line 9)) (4.3.3)\n",
      "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /opt/conda/lib/python3.7/site-packages (from ipywidgets==7.7.0->-r requirements.txt (line 9)) (7.13.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.6.0 in /root/.local/lib/python3.7/site-packages (from ipywidgets==7.7.0->-r requirements.txt (line 9)) (3.6.6)\n",
      "Requirement already satisfied: iniconfig in /root/.local/lib/python3.7/site-packages (from pytest==7.1.1->-r requirements.txt (line 11)) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from pytest==7.1.1->-r requirements.txt (line 11)) (1.5.0)\n",
      "Requirement already satisfied: py>=1.8.2 in /root/.local/lib/python3.7/site-packages (from pytest==7.1.1->-r requirements.txt (line 11)) (1.11.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /root/.local/lib/python3.7/site-packages (from pytest==7.1.1->-r requirements.txt (line 11)) (2.0.1)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /root/.local/lib/python3.7/site-packages (from pytest==7.1.1->-r requirements.txt (line 11)) (1.2.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.7/site-packages (from pytest==7.1.1->-r requirements.txt (line 11)) (19.3.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas==1.3.5->-r requirements.txt (line 12)) (2019.3)\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/conda/lib/python3.7/site-packages (from seaborn==0.11.2->-r requirements.txt (line 13)) (1.7.1)\n",
      "Requirement already satisfied: qtconsole in /root/.local/lib/python3.7/site-packages (from jupyter==1.0.0->-r requirements.txt (line 14)) (5.4.4)\n",
      "Requirement already satisfied: notebook in /opt/conda/lib/python3.7/site-packages (from jupyter==1.0.0->-r requirements.txt (line 14)) (5.7.4)\n",
      "Requirement already satisfied: jupyter-console in /root/.local/lib/python3.7/site-packages (from jupyter==1.0.0->-r requirements.txt (line 14)) (6.6.3)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/lib/python3.7/site-packages (from jupyter==1.0.0->-r requirements.txt (line 14)) (5.6.1)\n",
      "Requirement already satisfied: jupyter-client in /opt/conda/lib/python3.7/site-packages (from ipykernel==4.10.0->-r requirements.txt (line 15)) (6.0.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib==3.4.3->-r requirements.txt (line 2)) (45.2.0.post20200209)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from cycler>=0.10->matplotlib==3.4.3->-r requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.7/site-packages (from Jinja2>=2.7->bokeh==2.1.1->-r requirements.txt (line 5)) (1.1.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==0.12.0->-r requirements.txt (line 7)) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==0.12.0->-r requirements.txt (line 7)) (1.25.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==0.12.0->-r requirements.txt (line 7)) (2019.11.28)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==0.12.0->-r requirements.txt (line 7)) (3.0.4)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets==7.7.0->-r requirements.txt (line 9)) (3.2.0)\n",
      "Requirement already satisfied: jupyter-core in /opt/conda/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets==7.7.0->-r requirements.txt (line 9)) (4.6.3)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from traitlets>=4.3.1->ipywidgets==7.7.0->-r requirements.txt (line 9)) (4.4.2)\n",
      "Requirement already satisfied: jedi>=0.10 in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets==7.7.0->-r requirements.txt (line 9)) (0.16.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets==7.7.0->-r requirements.txt (line 9)) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets==7.7.0->-r requirements.txt (line 9)) (3.0.3)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets==7.7.0->-r requirements.txt (line 9)) (2.5.2)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets==7.7.0->-r requirements.txt (line 9)) (4.8.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets==7.7.0->-r requirements.txt (line 9)) (0.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest==7.1.1->-r requirements.txt (line 11)) (3.0.0)\n",
      "Requirement already satisfied: pyzmq>=17.1 in /opt/conda/lib/python3.7/site-packages (from qtconsole->jupyter==1.0.0->-r requirements.txt (line 14)) (19.0.0)\n",
      "Requirement already satisfied: qtpy>=2.4.0 in /root/.local/lib/python3.7/site-packages (from qtconsole->jupyter==1.0.0->-r requirements.txt (line 14)) (2.4.0)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.7/site-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 14)) (0.7.1)\n",
      "Requirement already satisfied: Send2Trash in /opt/conda/lib/python3.7/site-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 14)) (1.5.0)\n",
      "Requirement already satisfied: terminado>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 14)) (0.8.3)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 14)) (1.4.2)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.7/site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 14)) (0.6.0)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /opt/conda/lib/python3.7/site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 14)) (0.3)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.7/site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 14)) (0.4.4)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 14)) (0.8.4)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.7/site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 14)) (3.1.1)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets==7.7.0->-r requirements.txt (line 9)) (0.15.7)\n",
      "Requirement already satisfied: parso>=0.5.2 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.10->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets==7.7.0->-r requirements.txt (line 9)) (0.6.1)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets==7.7.0->-r requirements.txt (line 9)) (0.1.8)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets==7.7.0->-r requirements.txt (line 9)) (0.6.0)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.7/site-packages (from bleach->nbconvert->jupyter==1.0.0->-r requirements.txt (line 14)) (0.5.1)\n"
     ]
    }
   ],
   "source": [
    "# Install requirements\n",
    "!python -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This cell contains the essential imports you will need – DO NOT CHANGE THE CONTENTS! ##\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset\n",
    "\n",
    "Specify your transforms as a list if you intend to .\n",
    "The transforms module is already loaded as `transforms`.\n",
    "\n",
    "MNIST is fortunately included in the torchvision module.\n",
    "Then, you can create your dataset using the `MNIST` object from `torchvision.datasets` ([the documentation is available here](https://pytorch.org/vision/stable/datasets.html#mnist)).\n",
    "Make sure to specify `download=True`! \n",
    "\n",
    "Once your dataset is created, you'll also need to define a `DataLoader` from the `torch.utils.data` module for both the train and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from functools import partial\n",
    "\n",
    "# set ascii as default for all tqdm objects\n",
    "tqdm.tqdm = partial(tqdm.tqdm, ascii=True)\n",
    "\n",
    "\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]) \n",
    "\n",
    "# Create training set and define training dataloader\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "# Create test set and define test dataloader\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Justify your preprocessing\n",
    "\n",
    "In your own words, why did you choose the transforms you chose? If you didn't use any preprocessing steps, why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chosen transforms, ToTensor and Normalization, are fundamental preprocessing steps typically used in training neural network models, especially with image data. ToTensor transform converts the PIL Image or numpy.ndarray to a float32 torch tensor and scales the values to the range [0.0, 1.0], ensuring the neural network can process the inputs more effectively without disruptions or slowdowns in learning. Normalization, achieved using transforms.Normalize((0.5,), (0.5,)), adjusts the range of pixel intensity values to be between -1 and 1 by altering each channel of the input, promoting faster learning and improved performance due to the optimized process of the network. These transforms were specifically chosen for their simplicity and effectiveness, especially considering the straightforward nature of the MNIST dataset which involves grayscale images of digits and does not necessitate complex augmentations or additional preprocessing steps. While data augmentation like rotation or flipping wasn't employed here due to the simplicity of the dataset, it is a valuable technique for enhancing model generalization on more intricate and varied datasets by expanding the size and diversity of the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Dataset\n",
    "Using matplotlib, numpy, and torch, explore the dimensions of your data.\n",
    "\n",
    "You can view images using the `show5` function defined below – it takes a data loader as an argument.\n",
    "Remember that normalized images will look really weird to you! You may want to try changing your transforms to view images.\n",
    "Typically using no transforms other than `toTensor()` works well for viewing – but not as well for training your network.\n",
    "If `show5` doesn't work, go back and check your code for creating your data loaders and your training/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This cell contains a function for showing 5 images from a dataloader – DO NOT CHANGE THE CONTENTS! ##\n",
    "def show5(img_loader):\n",
    "    dataiter = iter(img_loader)\n",
    "    \n",
    "    batch = next(dataiter)\n",
    "    labels = batch[1][0:5]\n",
    "    images = batch[0][0:5]\n",
    "    for i in range(5):\n",
    "        print(int(labels[i].detach()))\n",
    "    \n",
    "        image = images[i].numpy()\n",
    "        plt.imshow(image.T.squeeze().T)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAORklEQVR4nO3df6zddX3H8deLUtpRIFAKpbSVXwKOLaGwm4ITNxyOQSMBoiM2jpRYLW6FAJJN4haEZJuMiIYxx1YHsxoEjULoTDPoGpTgRsMtq22hYLEr2tIfI6BUMy798d4f91tygfv9nNvzm/t+PpKbc873fb7n+86hL77f8/2c8/04IgRg/Duo1w0A6A7CDiRB2IEkCDuQBGEHkji4mxs7xJNisqZ0c5NAKq/r13ojhjxaraWw275I0p2SJkj6l4i4rfT8yZqic3xBK5sEULAqVtbWmj6Mtz1B0lclXSzpDEnzbZ/R7OsB6KxWPrPPlfRCRGyKiDckPSDp0va0BaDdWgn7TEk/H/F4S7XsLWwvsj1oe3C3hlrYHIBWdPxsfEQsiYiBiBiYqEmd3hyAGq2Efauk2SMez6qWAehDrYT9KUmn2j7J9iGSPi5pWXvaAtBuTQ+9RcQe29dIekTDQ2/3RsQzbesMQFu1NM4eEcslLW9TLwA6iK/LAkkQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5BEV6dsRj4Tph1dW3vx06cX1/2bT36jWF8y78Jife/GTcV6NuzZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJxtnRktcvmVusL77j27W1j05Z0dK2v3ju9GL9SMbZ36KlsNveLGmXpL2S9kTEQDuaAtB+7dizfygiXm7D6wDoID6zA0m0GvaQ9Kjt1bYXjfYE24tsD9oe3K2hFjcHoFmtHsafFxFbbR8raYXt5yLi8ZFPiIglkpZI0hGeGi1uD0CTWtqzR8TW6nanpIcklU/NAuiZpsNue4rtw/ffl3ShpPXtagxAe7VyGD9d0kO297/OtyLi39vSFbrm4BNmF+sbb5tarD/0/juL9fdNnHTAPY3V9E/9T7E+9M2ObfpdqemwR8QmSWe2sRcAHcTQG5AEYQeSIOxAEoQdSIKwA0nwE9dx7uBZM4v1l/5+SrG+YeBfG2yhPLR2+g8W1tZOu+5nxXUv/uELxfrRk35drL9UrObDnh1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkmCcfRw4aM4ZtbW5S9cU1/2raeVLEExweX/wwK6jivXTb6wf7d77i18W193yRvm1cWDYswNJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoyzjwMvXnJkbe37DcbRG5n3/LxiPW6o37Yk7dv+bG3toDN/s7ju3x57X7H+++s+VqxP0a5iPRv27EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBOPs44D31teeGoriup/+x2uL9Zl3rS7WY6j5q7M/d235mvWNvLzquGJ9ija19PrjTcM9u+17be+0vX7Esqm2V9jeWN1ylQGgz43lMP7rki5627KbJK2MiFMlraweA+hjDcMeEY9LeuVtiy+VtLS6v1TSZe1tC0C7NfuZfXpEbKvub5c0ve6JthdJWiRJk3Vok5sD0KqWz8ZHREiqPQsUEUsiYiAiBiY2mAQQQOc0G/YdtmdIUnW7s30tAeiEZsO+TNKC6v4CSQ+3px0AndLwM7vt+yWdL2ma7S2SviDpNknfsb1Q0ouSruhkkyh7z13ramu3fv8TxXWPX/ufxXp5lL41p520vaX1T1xWvu58J3t/N2oY9oiYX1O6oM29AOggvi4LJEHYgSQIO5AEYQeSIOxAEvzEdRzYt6twyeS1z3WvkVH88hPn1tZ++L67iuuufqP82hO2v1qs7ymvng57diAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgnF2dNRvXVs/ZfTBmlBcd+FXrynWj99a/nku3oo9O5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kwTg7WvJ/l80t1v/6+C/X1jY3+MH5zMe4VHQ7sWcHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQYZ38X2H7D7xbrH/6TJzu27R/tOLlY/+h7Vhbrx044tLZ2+rcWF9c9ZfV/Fes4MA337Lbvtb3T9voRy26xvdX2mupvXmfbBNCqsRzGf13SRaMs/0pEzKn+lre3LQDt1jDsEfG4pFe60AuADmrlBN01ttdWh/lH1T3J9iLbg7YHd2uohc0BaEWzYb9b0imS5kjaJumOuidGxJKIGIiIgYma1OTmALSqqbBHxI6I2BsR+yR9TVL5p08Aeq6psNueMeLh5ZLqrxcMoC80HGe3fb+k8yVNs71F0hcknW97joZ/UrxZ0tWda3H8e3XB+4v1H3z2S8X6EQdNbmc7b3XcYMde+vQ7NhXrzK/eXg3DHhHzR1l8Twd6AdBBfF0WSIKwA0kQdiAJwg4kQdiBJPiJaxdMOOaYYv32m/+5WJ/k8n+mM1ddWVu77OS1xXVvPebHxXonbf3jU4r16Xft6FInObBnB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkGGfvguduLl+O+YOTHynWz779+mJ91j+trq09+OcfLK5765/2bpz9ic/VT+csSWfNvqFYf+/N/12s73v99QPuaTxjzw4kQdiBJAg7kARhB5Ig7EAShB1IgrADSTgiuraxIzw1zvEFXdtev5j15GHF+o7XDy/W9/xReaq9n/3F79TW1n3mH4rrNnLJTz5SrL/04InF+vWLv1tbu/Lw7c209KYbtp1TrL+woP77DXufeb6lbferVbFSr8UrHq3Gnh1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkmCcvQ0OPm56sf7twYeL9fP+7rPF+tBR5e2vv7p+LH2P9hbXvejZjxXrkz5SHguPoaFi/aDD679D8PxdpxXXXf/hu4v1iZ5QrH/ouj+rrU357qriuu9WLY2z255t+zHbz9p+xvZ11fKptlfY3ljdNvgnCaCXxnIYv0fSjRFxhqRzJS22fYakmyStjIhTJa2sHgPoUw3DHhHbIuLp6v4uSRskzZR0qaSl1dOWSrqsQz0CaIMDugad7RMlnSVplaTpEbGtKm2XNOoHV9uLJC2SpMk6tOlGAbRmzGfjbR8m6XuSro+I10bWYvgs36hn+iJiSUQMRMTARE1qqVkAzRtT2G1P1HDQ74uIB6vFO2zPqOozJO3sTIsA2qHhYbxtS7pH0oaIGHnt32WSFki6rbotjy+NY1uvKE89/Bs+pFi/6jPLi/Vrj9x0wD3td+aPPlmsn3DFumK91YHZfbt21dZOvar+EtiSdPm5nyrWh44uHyke9mj9paa7N+DcP8bymf0Dkq6UtM72mmrZ5zUc8u/YXijpRUlXdKRDAG3RMOwR8YSkUQfpJY2/b8gA4xRflwWSIOxAEoQdSIKwA0kQdiAJpmxug0m/aG3UttE4eqOfqZbG0k9auLm47r5itceeXFssN/o+Zsax9BL27EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBOPsbXD0Iz8tP+GL5fKdr763WP+3G/+gWD/hkcHaWl+Po6Or2LMDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKMs7fB3h3l+THmzTy7pdc/RPXj6MBYsWcHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQaht32bNuP2X7W9jO2r6uW32J7q+011d+8zrcLoFlj+VLNHkk3RsTTtg+XtNr2iqr2lYj4UufaA9AuY5mffZukbdX9XbY3SJrZ6cYAtNcBfWa3faKksyStqhZdY3ut7XttH1WzziLbg7YHd2uotW4BNG3MYbd9mKTvSbo+Il6TdLekUyTN0fCe/47R1ouIJRExEBEDExvOzgWgU8YUdtsTNRz0+yLiQUmKiB0RsTci9kn6mqS5nWsTQKvGcjbeku6RtCEivjxi+YwRT7tc0vr2twegXcZyNv4Dkq6UtM72mmrZ5yXNtz1HwzPjbpZ0dQf6A9AmYzkb/4Qkj1Ja3v52AHQK36ADkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4k4Yjo3sbs/5X04ohF0yS93LUGDky/9tavfUn01qx29nZCRBwzWqGrYX/Hxu3BiBjoWQMF/dpbv/Yl0VuzutUbh/FAEoQdSKLXYV/S4+2X9Gtv/dqXRG/N6kpvPf3MDqB7er1nB9AlhB1Ioidht32R7edtv2D7pl70UMf2ZtvrqmmoB3vcy722d9peP2LZVNsrbG+sbkedY69HvfXFNN6FacZ7+t71evrzrn9mtz1B0k8k/aGkLZKekjQ/Ip7taiM1bG+WNBARPf8Chu3fk/QrSd+IiN+ult0u6ZWIuK36H+VREfG5PuntFkm/6vU03tVsRTNGTjMu6TJJV6mH712hryvUhfetF3v2uZJeiIhNEfGGpAckXdqDPvpeRDwu6ZW3Lb5U0tLq/lIN/2Ppupre+kJEbIuIp6v7uyTtn2a8p+9doa+u6EXYZ0r6+YjHW9Rf872HpEdtr7a9qNfNjGJ6RGyr7m+XNL2XzYyi4TTe3fS2acb75r1rZvrzVnGC7p3Oi4izJV0saXF1uNqXYvgzWD+NnY5pGu9uGWWa8Tf18r1rdvrzVvUi7FslzR7xeFa1rC9ExNbqdqekh9R/U1Hv2D+DbnW7s8f9vKmfpvEebZpx9cF718vpz3sR9qcknWr7JNuHSPq4pGU96OMdbE+pTpzI9hRJF6r/pqJeJmlBdX+BpId72Mtb9Ms03nXTjKvH713Ppz+PiK7/SZqn4TPyP5X0l73ooaavkyX9uPp7pte9Sbpfw4d1uzV8bmOhpKMlrZS0UdJ/SJraR719U9I6SWs1HKwZPertPA0foq+VtKb6m9fr967QV1feN74uCyTBCTogCcIOJEHYgSQIO5AEYQeSIOxAEoQdSOL/AYCkLliK99maAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a transform for viewing that only converts the images to tensors\n",
    "view_transform = transforms.Compose([transforms.ToTensor()]) \n",
    "\n",
    "# Create a viewing dataset and dataloader with the new transform\n",
    "view_trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=view_transform)\n",
    "view_trainloader = torch.utils.data.DataLoader(view_trainset, batch_size=5, shuffle=True, num_workers=2)\n",
    "\n",
    "# Use the show5 function to visualize the data\n",
    "show5(view_trainloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build your Neural Network\n",
    "Using the layers in `torch.nn` (which has been imported as `nn`) and the `torch.nn.functional` module (imported as `F`), construct a neural network based on the parameters of the dataset.\n",
    "Use any architecture you like. \n",
    "\n",
    "*Note*: If you did not flatten your tensors in your transforms or as part of your preprocessing and you are using only `Linear` layers, make sure to use the `Flatten` layer in your network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.flatten = nn.Flatten()  # Flatten the 28x28 image\n",
    "        self.fc1 = nn.Linear(28*28, 128)  # First fully connected layer\n",
    "        self.fc2 = nn.Linear(128, 10)  # Second fully connected layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)  # Flatten the input\n",
    "        x = F.relu(self.fc1(x))  # Apply ReLU activation function after the first fully connected layer\n",
    "        x = self.fc2(x)  # Pass through the second fully connected layer\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = SimpleNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify a loss function and an optimizer, and instantiate the model.\n",
    "\n",
    "If you use a less common loss function, please note why you chose that loss function in a comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify Loss Function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Specify Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running your Neural Network\n",
    "Use whatever method you like to train your neural network, and ensure you record the average loss at each epoch. \n",
    "Don't forget to use `torch.device()` and the `.to()` method for both your model and your data if you are using GPU!\n",
    "\n",
    "If you want to print your loss **during** each epoch, you can use the `enumerate` function and print the loss after a set number of batches. 250 batches works well for most people!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n",
      "Epoch [1/10], Step [250/15000], Loss: 1.1946\n",
      "Epoch [1/10], Step [500/15000], Loss: 0.5772\n",
      "Epoch [1/10], Step [750/15000], Loss: 0.5336\n",
      "Epoch [1/10], Step [1000/15000], Loss: 0.4976\n",
      "Epoch [1/10], Step [1250/15000], Loss: 0.4626\n",
      "Epoch [1/10], Step [1500/15000], Loss: 0.5459\n",
      "Epoch [1/10], Step [1750/15000], Loss: 0.4627\n",
      "Epoch [1/10], Step [2000/15000], Loss: 0.3568\n",
      "Epoch [1/10], Step [2250/15000], Loss: 0.3819\n",
      "Epoch [1/10], Step [2500/15000], Loss: 0.3147\n",
      "Epoch [1/10], Step [2750/15000], Loss: 0.3808\n",
      "Epoch [1/10], Step [3000/15000], Loss: 0.3653\n",
      "Epoch [1/10], Step [3250/15000], Loss: 0.4270\n",
      "Epoch [1/10], Step [3500/15000], Loss: 0.2961\n",
      "Epoch [1/10], Step [3750/15000], Loss: 0.3361\n",
      "Epoch [1/10], Step [4000/15000], Loss: 0.3795\n",
      "Epoch [1/10], Step [4250/15000], Loss: 0.3206\n",
      "Epoch [1/10], Step [4500/15000], Loss: 0.3179\n",
      "Epoch [1/10], Step [4750/15000], Loss: 0.3349\n",
      "Epoch [1/10], Step [5000/15000], Loss: 0.3312\n",
      "Epoch [1/10], Step [5250/15000], Loss: 0.3111\n",
      "Epoch [1/10], Step [5500/15000], Loss: 0.3000\n",
      "Epoch [1/10], Step [5750/15000], Loss: 0.3294\n",
      "Epoch [1/10], Step [6000/15000], Loss: 0.3317\n",
      "Epoch [1/10], Step [6250/15000], Loss: 0.2400\n",
      "Epoch [1/10], Step [6500/15000], Loss: 0.2845\n",
      "Epoch [1/10], Step [6750/15000], Loss: 0.2932\n",
      "Epoch [1/10], Step [7000/15000], Loss: 0.2545\n",
      "Epoch [1/10], Step [7250/15000], Loss: 0.2395\n",
      "Epoch [1/10], Step [7500/15000], Loss: 0.2966\n",
      "Epoch [1/10], Step [7750/15000], Loss: 0.2479\n",
      "Epoch [1/10], Step [8000/15000], Loss: 0.2642\n",
      "Epoch [1/10], Step [8250/15000], Loss: 0.2339\n",
      "Epoch [1/10], Step [8500/15000], Loss: 0.2811\n",
      "Epoch [1/10], Step [8750/15000], Loss: 0.2911\n",
      "Epoch [1/10], Step [9000/15000], Loss: 0.3021\n",
      "Epoch [1/10], Step [9250/15000], Loss: 0.2742\n",
      "Epoch [1/10], Step [9500/15000], Loss: 0.2531\n",
      "Epoch [1/10], Step [9750/15000], Loss: 0.2620\n",
      "Epoch [1/10], Step [10000/15000], Loss: 0.2346\n",
      "Epoch [1/10], Step [10250/15000], Loss: 0.2763\n",
      "Epoch [1/10], Step [10500/15000], Loss: 0.2709\n",
      "Epoch [1/10], Step [10750/15000], Loss: 0.2627\n",
      "Epoch [1/10], Step [11000/15000], Loss: 0.2070\n",
      "Epoch [1/10], Step [11250/15000], Loss: 0.1949\n",
      "Epoch [1/10], Step [11500/15000], Loss: 0.2282\n",
      "Epoch [1/10], Step [11750/15000], Loss: 0.2008\n",
      "Epoch [1/10], Step [12000/15000], Loss: 0.2548\n",
      "Epoch [1/10], Step [12250/15000], Loss: 0.2163\n",
      "Epoch [1/10], Step [12500/15000], Loss: 0.2393\n",
      "Epoch [1/10], Step [12750/15000], Loss: 0.2367\n",
      "Epoch [1/10], Step [13000/15000], Loss: 0.2270\n",
      "Epoch [1/10], Step [13250/15000], Loss: 0.2736\n",
      "Epoch [1/10], Step [13500/15000], Loss: 0.2137\n",
      "Epoch [1/10], Step [13750/15000], Loss: 0.2450\n",
      "Epoch [1/10], Step [14000/15000], Loss: 0.1755\n",
      "Epoch [1/10], Step [14250/15000], Loss: 0.2400\n",
      "Epoch [1/10], Step [14500/15000], Loss: 0.2114\n",
      "Epoch [1/10], Step [14750/15000], Loss: 0.1718\n",
      "Epoch [1/10], Step [15000/15000], Loss: 0.2574\n",
      "Epoch [1/10], Average Loss: 0.0000\n",
      "Epoch [2/10], Step [250/15000], Loss: 0.1812\n",
      "Epoch [2/10], Step [500/15000], Loss: 0.2548\n",
      "Epoch [2/10], Step [750/15000], Loss: 0.1913\n",
      "Epoch [2/10], Step [1000/15000], Loss: 0.2027\n",
      "Epoch [2/10], Step [1250/15000], Loss: 0.1992\n",
      "Epoch [2/10], Step [1500/15000], Loss: 0.2175\n",
      "Epoch [2/10], Step [1750/15000], Loss: 0.2218\n",
      "Epoch [2/10], Step [2000/15000], Loss: 0.1621\n",
      "Epoch [2/10], Step [2250/15000], Loss: 0.1712\n",
      "Epoch [2/10], Step [2500/15000], Loss: 0.1903\n",
      "Epoch [2/10], Step [2750/15000], Loss: 0.1864\n",
      "Epoch [2/10], Step [3000/15000], Loss: 0.1725\n",
      "Epoch [2/10], Step [3250/15000], Loss: 0.2168\n",
      "Epoch [2/10], Step [3500/15000], Loss: 0.1589\n",
      "Epoch [2/10], Step [3750/15000], Loss: 0.1955\n",
      "Epoch [2/10], Step [4000/15000], Loss: 0.2337\n",
      "Epoch [2/10], Step [4250/15000], Loss: 0.2003\n",
      "Epoch [2/10], Step [4500/15000], Loss: 0.2120\n",
      "Epoch [2/10], Step [4750/15000], Loss: 0.1960\n",
      "Epoch [2/10], Step [5000/15000], Loss: 0.1361\n",
      "Epoch [2/10], Step [5250/15000], Loss: 0.1647\n",
      "Epoch [2/10], Step [5500/15000], Loss: 0.2006\n",
      "Epoch [2/10], Step [5750/15000], Loss: 0.1584\n",
      "Epoch [2/10], Step [6000/15000], Loss: 0.2130\n",
      "Epoch [2/10], Step [6250/15000], Loss: 0.1702\n",
      "Epoch [2/10], Step [6500/15000], Loss: 0.1649\n",
      "Epoch [2/10], Step [6750/15000], Loss: 0.1589\n",
      "Epoch [2/10], Step [7000/15000], Loss: 0.2201\n",
      "Epoch [2/10], Step [7250/15000], Loss: 0.2172\n",
      "Epoch [2/10], Step [7500/15000], Loss: 0.1870\n",
      "Epoch [2/10], Step [7750/15000], Loss: 0.1654\n",
      "Epoch [2/10], Step [8000/15000], Loss: 0.1583\n",
      "Epoch [2/10], Step [8250/15000], Loss: 0.1796\n",
      "Epoch [2/10], Step [8500/15000], Loss: 0.1578\n",
      "Epoch [2/10], Step [8750/15000], Loss: 0.2119\n",
      "Epoch [2/10], Step [9000/15000], Loss: 0.1847\n",
      "Epoch [2/10], Step [9250/15000], Loss: 0.2024\n",
      "Epoch [2/10], Step [9500/15000], Loss: 0.2599\n",
      "Epoch [2/10], Step [9750/15000], Loss: 0.1376\n",
      "Epoch [2/10], Step [10000/15000], Loss: 0.1936\n",
      "Epoch [2/10], Step [10250/15000], Loss: 0.1649\n",
      "Epoch [2/10], Step [10500/15000], Loss: 0.2057\n",
      "Epoch [2/10], Step [10750/15000], Loss: 0.2586\n",
      "Epoch [2/10], Step [11000/15000], Loss: 0.1350\n",
      "Epoch [2/10], Step [11250/15000], Loss: 0.1443\n",
      "Epoch [2/10], Step [11500/15000], Loss: 0.1794\n",
      "Epoch [2/10], Step [11750/15000], Loss: 0.2015\n",
      "Epoch [2/10], Step [12000/15000], Loss: 0.1948\n",
      "Epoch [2/10], Step [12250/15000], Loss: 0.1539\n",
      "Epoch [2/10], Step [12500/15000], Loss: 0.1924\n",
      "Epoch [2/10], Step [12750/15000], Loss: 0.1812\n",
      "Epoch [2/10], Step [13000/15000], Loss: 0.1948\n",
      "Epoch [2/10], Step [13250/15000], Loss: 0.1431\n",
      "Epoch [2/10], Step [13500/15000], Loss: 0.1731\n",
      "Epoch [2/10], Step [13750/15000], Loss: 0.1833\n",
      "Epoch [2/10], Step [14000/15000], Loss: 0.2108\n",
      "Epoch [2/10], Step [14250/15000], Loss: 0.1534\n",
      "Epoch [2/10], Step [14500/15000], Loss: 0.1480\n",
      "Epoch [2/10], Step [14750/15000], Loss: 0.1876\n",
      "Epoch [2/10], Step [15000/15000], Loss: 0.1746\n",
      "Epoch [2/10], Average Loss: 0.0000\n",
      "Epoch [3/10], Step [250/15000], Loss: 0.1641\n",
      "Epoch [3/10], Step [500/15000], Loss: 0.1469\n",
      "Epoch [3/10], Step [750/15000], Loss: 0.1336\n",
      "Epoch [3/10], Step [1000/15000], Loss: 0.1306\n",
      "Epoch [3/10], Step [1250/15000], Loss: 0.1691\n",
      "Epoch [3/10], Step [1500/15000], Loss: 0.1392\n",
      "Epoch [3/10], Step [1750/15000], Loss: 0.1087\n",
      "Epoch [3/10], Step [2000/15000], Loss: 0.1659\n",
      "Epoch [3/10], Step [2250/15000], Loss: 0.1711\n",
      "Epoch [3/10], Step [2500/15000], Loss: 0.1140\n",
      "Epoch [3/10], Step [2750/15000], Loss: 0.2058\n",
      "Epoch [3/10], Step [3000/15000], Loss: 0.1645\n",
      "Epoch [3/10], Step [3250/15000], Loss: 0.2161\n",
      "Epoch [3/10], Step [3500/15000], Loss: 0.1613\n",
      "Epoch [3/10], Step [3750/15000], Loss: 0.1726\n",
      "Epoch [3/10], Step [4000/15000], Loss: 0.0956\n",
      "Epoch [3/10], Step [4250/15000], Loss: 0.1707\n",
      "Epoch [3/10], Step [4500/15000], Loss: 0.1940\n",
      "Epoch [3/10], Step [4750/15000], Loss: 0.1871\n",
      "Epoch [3/10], Step [5000/15000], Loss: 0.1568\n",
      "Epoch [3/10], Step [5250/15000], Loss: 0.1762\n",
      "Epoch [3/10], Step [5500/15000], Loss: 0.1450\n",
      "Epoch [3/10], Step [5750/15000], Loss: 0.2014\n",
      "Epoch [3/10], Step [6000/15000], Loss: 0.1666\n",
      "Epoch [3/10], Step [6250/15000], Loss: 0.1296\n",
      "Epoch [3/10], Step [6500/15000], Loss: 0.1821\n",
      "Epoch [3/10], Step [6750/15000], Loss: 0.1143\n",
      "Epoch [3/10], Step [7000/15000], Loss: 0.1127\n",
      "Epoch [3/10], Step [7250/15000], Loss: 0.1770\n",
      "Epoch [3/10], Step [7500/15000], Loss: 0.1557\n",
      "Epoch [3/10], Step [7750/15000], Loss: 0.1272\n",
      "Epoch [3/10], Step [8000/15000], Loss: 0.2560\n",
      "Epoch [3/10], Step [8250/15000], Loss: 0.1538\n",
      "Epoch [3/10], Step [8500/15000], Loss: 0.1709\n",
      "Epoch [3/10], Step [8750/15000], Loss: 0.1337\n",
      "Epoch [3/10], Step [9000/15000], Loss: 0.1397\n",
      "Epoch [3/10], Step [9250/15000], Loss: 0.2416\n",
      "Epoch [3/10], Step [9500/15000], Loss: 0.1222\n",
      "Epoch [3/10], Step [9750/15000], Loss: 0.1825\n",
      "Epoch [3/10], Step [10000/15000], Loss: 0.1172\n",
      "Epoch [3/10], Step [10250/15000], Loss: 0.1353\n",
      "Epoch [3/10], Step [10500/15000], Loss: 0.1417\n",
      "Epoch [3/10], Step [10750/15000], Loss: 0.1695\n",
      "Epoch [3/10], Step [11000/15000], Loss: 0.1448\n",
      "Epoch [3/10], Step [11250/15000], Loss: 0.1662\n",
      "Epoch [3/10], Step [11500/15000], Loss: 0.1653\n",
      "Epoch [3/10], Step [11750/15000], Loss: 0.1232\n",
      "Epoch [3/10], Step [12000/15000], Loss: 0.1261\n",
      "Epoch [3/10], Step [12250/15000], Loss: 0.2025\n",
      "Epoch [3/10], Step [12500/15000], Loss: 0.1965\n",
      "Epoch [3/10], Step [12750/15000], Loss: 0.1861\n",
      "Epoch [3/10], Step [13000/15000], Loss: 0.1654\n",
      "Epoch [3/10], Step [13250/15000], Loss: 0.1628\n",
      "Epoch [3/10], Step [13500/15000], Loss: 0.1772\n",
      "Epoch [3/10], Step [13750/15000], Loss: 0.1706\n",
      "Epoch [3/10], Step [14000/15000], Loss: 0.1761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Step [14250/15000], Loss: 0.1540\n",
      "Epoch [3/10], Step [14500/15000], Loss: 0.1661\n",
      "Epoch [3/10], Step [14750/15000], Loss: 0.1671\n",
      "Epoch [3/10], Step [15000/15000], Loss: 0.1816\n",
      "Epoch [3/10], Average Loss: 0.0000\n",
      "Epoch [4/10], Step [250/15000], Loss: 0.1073\n",
      "Epoch [4/10], Step [500/15000], Loss: 0.1413\n",
      "Epoch [4/10], Step [750/15000], Loss: 0.1351\n",
      "Epoch [4/10], Step [1000/15000], Loss: 0.1196\n",
      "Epoch [4/10], Step [1250/15000], Loss: 0.1404\n",
      "Epoch [4/10], Step [1500/15000], Loss: 0.1701\n",
      "Epoch [4/10], Step [1750/15000], Loss: 0.1329\n",
      "Epoch [4/10], Step [2000/15000], Loss: 0.1411\n",
      "Epoch [4/10], Step [2250/15000], Loss: 0.1805\n",
      "Epoch [4/10], Step [2500/15000], Loss: 0.1388\n",
      "Epoch [4/10], Step [2750/15000], Loss: 0.1396\n",
      "Epoch [4/10], Step [3000/15000], Loss: 0.1555\n",
      "Epoch [4/10], Step [3250/15000], Loss: 0.1331\n",
      "Epoch [4/10], Step [3500/15000], Loss: 0.1737\n",
      "Epoch [4/10], Step [3750/15000], Loss: 0.1553\n",
      "Epoch [4/10], Step [4000/15000], Loss: 0.0981\n",
      "Epoch [4/10], Step [4250/15000], Loss: 0.1725\n",
      "Epoch [4/10], Step [4500/15000], Loss: 0.1345\n",
      "Epoch [4/10], Step [4750/15000], Loss: 0.1259\n",
      "Epoch [4/10], Step [5000/15000], Loss: 0.1171\n",
      "Epoch [4/10], Step [5250/15000], Loss: 0.1517\n",
      "Epoch [4/10], Step [5500/15000], Loss: 0.1443\n",
      "Epoch [4/10], Step [5750/15000], Loss: 0.1289\n",
      "Epoch [4/10], Step [6000/15000], Loss: 0.1405\n",
      "Epoch [4/10], Step [6250/15000], Loss: 0.1092\n",
      "Epoch [4/10], Step [6500/15000], Loss: 0.1409\n",
      "Epoch [4/10], Step [6750/15000], Loss: 0.1528\n",
      "Epoch [4/10], Step [7000/15000], Loss: 0.1701\n",
      "Epoch [4/10], Step [7250/15000], Loss: 0.1466\n",
      "Epoch [4/10], Step [7500/15000], Loss: 0.1540\n",
      "Epoch [4/10], Step [7750/15000], Loss: 0.1586\n",
      "Epoch [4/10], Step [8000/15000], Loss: 0.1166\n",
      "Epoch [4/10], Step [8250/15000], Loss: 0.1973\n",
      "Epoch [4/10], Step [8500/15000], Loss: 0.1085\n",
      "Epoch [4/10], Step [8750/15000], Loss: 0.1380\n",
      "Epoch [4/10], Step [9000/15000], Loss: 0.1463\n",
      "Epoch [4/10], Step [9250/15000], Loss: 0.1600\n",
      "Epoch [4/10], Step [9500/15000], Loss: 0.1560\n",
      "Epoch [4/10], Step [9750/15000], Loss: 0.1566\n",
      "Epoch [4/10], Step [10000/15000], Loss: 0.1770\n",
      "Epoch [4/10], Step [10250/15000], Loss: 0.1444\n",
      "Epoch [4/10], Step [10500/15000], Loss: 0.1614\n",
      "Epoch [4/10], Step [10750/15000], Loss: 0.1416\n",
      "Epoch [4/10], Step [11000/15000], Loss: 0.0836\n",
      "Epoch [4/10], Step [11250/15000], Loss: 0.1586\n",
      "Epoch [4/10], Step [11500/15000], Loss: 0.1468\n",
      "Epoch [4/10], Step [11750/15000], Loss: 0.1107\n",
      "Epoch [4/10], Step [12000/15000], Loss: 0.1578\n",
      "Epoch [4/10], Step [12250/15000], Loss: 0.1038\n",
      "Epoch [4/10], Step [12500/15000], Loss: 0.1303\n",
      "Epoch [4/10], Step [12750/15000], Loss: 0.1707\n",
      "Epoch [4/10], Step [13000/15000], Loss: 0.1255\n",
      "Epoch [4/10], Step [13250/15000], Loss: 0.1941\n",
      "Epoch [4/10], Step [13500/15000], Loss: 0.0954\n",
      "Epoch [4/10], Step [13750/15000], Loss: 0.1282\n",
      "Epoch [4/10], Step [14000/15000], Loss: 0.1439\n",
      "Epoch [4/10], Step [14250/15000], Loss: 0.1673\n",
      "Epoch [4/10], Step [14500/15000], Loss: 0.1709\n",
      "Epoch [4/10], Step [14750/15000], Loss: 0.1341\n",
      "Epoch [4/10], Step [15000/15000], Loss: 0.1320\n",
      "Epoch [4/10], Average Loss: 0.0000\n",
      "Epoch [5/10], Step [250/15000], Loss: 0.0941\n",
      "Epoch [5/10], Step [500/15000], Loss: 0.0969\n",
      "Epoch [5/10], Step [750/15000], Loss: 0.1494\n",
      "Epoch [5/10], Step [1000/15000], Loss: 0.1235\n",
      "Epoch [5/10], Step [1250/15000], Loss: 0.1361\n",
      "Epoch [5/10], Step [1500/15000], Loss: 0.1554\n",
      "Epoch [5/10], Step [1750/15000], Loss: 0.1214\n",
      "Epoch [5/10], Step [2000/15000], Loss: 0.1258\n",
      "Epoch [5/10], Step [2250/15000], Loss: 0.1062\n",
      "Epoch [5/10], Step [2500/15000], Loss: 0.1038\n",
      "Epoch [5/10], Step [2750/15000], Loss: 0.1134\n",
      "Epoch [5/10], Step [3000/15000], Loss: 0.1405\n",
      "Epoch [5/10], Step [3250/15000], Loss: 0.1102\n",
      "Epoch [5/10], Step [3500/15000], Loss: 0.1536\n",
      "Epoch [5/10], Step [3750/15000], Loss: 0.1551\n",
      "Epoch [5/10], Step [4000/15000], Loss: 0.1035\n",
      "Epoch [5/10], Step [4250/15000], Loss: 0.1331\n",
      "Epoch [5/10], Step [4500/15000], Loss: 0.1511\n",
      "Epoch [5/10], Step [4750/15000], Loss: 0.1156\n",
      "Epoch [5/10], Step [5000/15000], Loss: 0.1401\n",
      "Epoch [5/10], Step [5250/15000], Loss: 0.1366\n",
      "Epoch [5/10], Step [5500/15000], Loss: 0.1750\n",
      "Epoch [5/10], Step [5750/15000], Loss: 0.1611\n",
      "Epoch [5/10], Step [6000/15000], Loss: 0.1127\n",
      "Epoch [5/10], Step [6250/15000], Loss: 0.1375\n",
      "Epoch [5/10], Step [6500/15000], Loss: 0.1152\n",
      "Epoch [5/10], Step [6750/15000], Loss: 0.1192\n",
      "Epoch [5/10], Step [7000/15000], Loss: 0.1079\n",
      "Epoch [5/10], Step [7250/15000], Loss: 0.1325\n",
      "Epoch [5/10], Step [7500/15000], Loss: 0.1159\n",
      "Epoch [5/10], Step [7750/15000], Loss: 0.1365\n",
      "Epoch [5/10], Step [8000/15000], Loss: 0.1315\n",
      "Epoch [5/10], Step [8250/15000], Loss: 0.1075\n",
      "Epoch [5/10], Step [8500/15000], Loss: 0.1882\n",
      "Epoch [5/10], Step [8750/15000], Loss: 0.1169\n",
      "Epoch [5/10], Step [9000/15000], Loss: 0.1437\n",
      "Epoch [5/10], Step [9250/15000], Loss: 0.1491\n",
      "Epoch [5/10], Step [9500/15000], Loss: 0.1020\n",
      "Epoch [5/10], Step [9750/15000], Loss: 0.1272\n",
      "Epoch [5/10], Step [10000/15000], Loss: 0.1248\n",
      "Epoch [5/10], Step [10250/15000], Loss: 0.1441\n",
      "Epoch [5/10], Step [10500/15000], Loss: 0.1576\n",
      "Epoch [5/10], Step [10750/15000], Loss: 0.1694\n",
      "Epoch [5/10], Step [11000/15000], Loss: 0.1134\n",
      "Epoch [5/10], Step [11250/15000], Loss: 0.1201\n",
      "Epoch [5/10], Step [11500/15000], Loss: 0.0952\n",
      "Epoch [5/10], Step [11750/15000], Loss: 0.1533\n",
      "Epoch [5/10], Step [12000/15000], Loss: 0.1966\n",
      "Epoch [5/10], Step [12250/15000], Loss: 0.1172\n",
      "Epoch [5/10], Step [12500/15000], Loss: 0.1773\n",
      "Epoch [5/10], Step [12750/15000], Loss: 0.1007\n",
      "Epoch [5/10], Step [13000/15000], Loss: 0.1110\n",
      "Epoch [5/10], Step [13250/15000], Loss: 0.1444\n",
      "Epoch [5/10], Step [13500/15000], Loss: 0.0978\n",
      "Epoch [5/10], Step [13750/15000], Loss: 0.1144\n",
      "Epoch [5/10], Step [14000/15000], Loss: 0.1709\n",
      "Epoch [5/10], Step [14250/15000], Loss: 0.0988\n",
      "Epoch [5/10], Step [14500/15000], Loss: 0.1466\n",
      "Epoch [5/10], Step [14750/15000], Loss: 0.1018\n",
      "Epoch [5/10], Step [15000/15000], Loss: 0.1329\n",
      "Epoch [5/10], Average Loss: 0.0000\n",
      "Epoch [6/10], Step [250/15000], Loss: 0.1495\n",
      "Epoch [6/10], Step [500/15000], Loss: 0.0947\n",
      "Epoch [6/10], Step [750/15000], Loss: 0.1209\n",
      "Epoch [6/10], Step [1000/15000], Loss: 0.0774\n",
      "Epoch [6/10], Step [1250/15000], Loss: 0.1340\n",
      "Epoch [6/10], Step [1500/15000], Loss: 0.1122\n",
      "Epoch [6/10], Step [1750/15000], Loss: 0.1439\n",
      "Epoch [6/10], Step [2000/15000], Loss: 0.1235\n",
      "Epoch [6/10], Step [2250/15000], Loss: 0.0853\n",
      "Epoch [6/10], Step [2500/15000], Loss: 0.1410\n",
      "Epoch [6/10], Step [2750/15000], Loss: 0.1322\n",
      "Epoch [6/10], Step [3000/15000], Loss: 0.1165\n",
      "Epoch [6/10], Step [3250/15000], Loss: 0.1299\n",
      "Epoch [6/10], Step [3500/15000], Loss: 0.1449\n",
      "Epoch [6/10], Step [3750/15000], Loss: 0.1497\n",
      "Epoch [6/10], Step [4000/15000], Loss: 0.1361\n",
      "Epoch [6/10], Step [4250/15000], Loss: 0.1285\n",
      "Epoch [6/10], Step [4500/15000], Loss: 0.1276\n",
      "Epoch [6/10], Step [4750/15000], Loss: 0.1300\n",
      "Epoch [6/10], Step [5000/15000], Loss: 0.1352\n",
      "Epoch [6/10], Step [5250/15000], Loss: 0.0780\n",
      "Epoch [6/10], Step [5500/15000], Loss: 0.0849\n",
      "Epoch [6/10], Step [5750/15000], Loss: 0.1539\n",
      "Epoch [6/10], Step [6000/15000], Loss: 0.0802\n",
      "Epoch [6/10], Step [6250/15000], Loss: 0.1040\n",
      "Epoch [6/10], Step [6500/15000], Loss: 0.0890\n",
      "Epoch [6/10], Step [6750/15000], Loss: 0.1157\n",
      "Epoch [6/10], Step [7000/15000], Loss: 0.1731\n",
      "Epoch [6/10], Step [7250/15000], Loss: 0.1717\n",
      "Epoch [6/10], Step [7500/15000], Loss: 0.1025\n",
      "Epoch [6/10], Step [7750/15000], Loss: 0.1835\n",
      "Epoch [6/10], Step [8000/15000], Loss: 0.1332\n",
      "Epoch [6/10], Step [8250/15000], Loss: 0.1147\n",
      "Epoch [6/10], Step [8500/15000], Loss: 0.1524\n",
      "Epoch [6/10], Step [8750/15000], Loss: 0.1120\n",
      "Epoch [6/10], Step [9000/15000], Loss: 0.1249\n",
      "Epoch [6/10], Step [9250/15000], Loss: 0.1159\n",
      "Epoch [6/10], Step [9500/15000], Loss: 0.1280\n",
      "Epoch [6/10], Step [9750/15000], Loss: 0.1337\n",
      "Epoch [6/10], Step [10000/15000], Loss: 0.1257\n",
      "Epoch [6/10], Step [10250/15000], Loss: 0.1217\n",
      "Epoch [6/10], Step [10500/15000], Loss: 0.1371\n",
      "Epoch [6/10], Step [10750/15000], Loss: 0.1055\n",
      "Epoch [6/10], Step [11000/15000], Loss: 0.1382\n",
      "Epoch [6/10], Step [11250/15000], Loss: 0.1200\n",
      "Epoch [6/10], Step [11500/15000], Loss: 0.1939\n",
      "Epoch [6/10], Step [11750/15000], Loss: 0.1552\n",
      "Epoch [6/10], Step [12000/15000], Loss: 0.0861\n",
      "Epoch [6/10], Step [12250/15000], Loss: 0.0862\n",
      "Epoch [6/10], Step [12500/15000], Loss: 0.1822\n",
      "Epoch [6/10], Step [12750/15000], Loss: 0.1413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Step [13000/15000], Loss: 0.1200\n",
      "Epoch [6/10], Step [13250/15000], Loss: 0.1139\n",
      "Epoch [6/10], Step [13500/15000], Loss: 0.1446\n",
      "Epoch [6/10], Step [13750/15000], Loss: 0.1362\n",
      "Epoch [6/10], Step [14000/15000], Loss: 0.0956\n",
      "Epoch [6/10], Step [14250/15000], Loss: 0.1208\n",
      "Epoch [6/10], Step [14500/15000], Loss: 0.1109\n",
      "Epoch [6/10], Step [14750/15000], Loss: 0.1013\n",
      "Epoch [6/10], Step [15000/15000], Loss: 0.0925\n",
      "Epoch [6/10], Average Loss: 0.0000\n",
      "Epoch [7/10], Step [250/15000], Loss: 0.0640\n",
      "Epoch [7/10], Step [500/15000], Loss: 0.0952\n",
      "Epoch [7/10], Step [750/15000], Loss: 0.1119\n",
      "Epoch [7/10], Step [1000/15000], Loss: 0.0771\n",
      "Epoch [7/10], Step [1250/15000], Loss: 0.0846\n",
      "Epoch [7/10], Step [1500/15000], Loss: 0.1050\n",
      "Epoch [7/10], Step [1750/15000], Loss: 0.0817\n",
      "Epoch [7/10], Step [2000/15000], Loss: 0.1030\n",
      "Epoch [7/10], Step [2250/15000], Loss: 0.1461\n",
      "Epoch [7/10], Step [2500/15000], Loss: 0.1166\n",
      "Epoch [7/10], Step [2750/15000], Loss: 0.0825\n",
      "Epoch [7/10], Step [3000/15000], Loss: 0.0892\n",
      "Epoch [7/10], Step [3250/15000], Loss: 0.1651\n",
      "Epoch [7/10], Step [3500/15000], Loss: 0.1240\n",
      "Epoch [7/10], Step [3750/15000], Loss: 0.1251\n",
      "Epoch [7/10], Step [4000/15000], Loss: 0.1297\n",
      "Epoch [7/10], Step [4250/15000], Loss: 0.0974\n",
      "Epoch [7/10], Step [4500/15000], Loss: 0.1018\n",
      "Epoch [7/10], Step [4750/15000], Loss: 0.1232\n",
      "Epoch [7/10], Step [5000/15000], Loss: 0.1408\n",
      "Epoch [7/10], Step [5250/15000], Loss: 0.0781\n",
      "Epoch [7/10], Step [5500/15000], Loss: 0.1845\n",
      "Epoch [7/10], Step [5750/15000], Loss: 0.1007\n",
      "Epoch [7/10], Step [6000/15000], Loss: 0.1197\n",
      "Epoch [7/10], Step [6250/15000], Loss: 0.1608\n",
      "Epoch [7/10], Step [6500/15000], Loss: 0.1530\n",
      "Epoch [7/10], Step [6750/15000], Loss: 0.1442\n",
      "Epoch [7/10], Step [7000/15000], Loss: 0.1393\n",
      "Epoch [7/10], Step [7250/15000], Loss: 0.1544\n",
      "Epoch [7/10], Step [7500/15000], Loss: 0.1670\n",
      "Epoch [7/10], Step [7750/15000], Loss: 0.0886\n",
      "Epoch [7/10], Step [8000/15000], Loss: 0.1090\n",
      "Epoch [7/10], Step [8250/15000], Loss: 0.0845\n",
      "Epoch [7/10], Step [8500/15000], Loss: 0.1426\n",
      "Epoch [7/10], Step [8750/15000], Loss: 0.0999\n",
      "Epoch [7/10], Step [9000/15000], Loss: 0.1861\n",
      "Epoch [7/10], Step [9250/15000], Loss: 0.0849\n",
      "Epoch [7/10], Step [9500/15000], Loss: 0.1002\n",
      "Epoch [7/10], Step [9750/15000], Loss: 0.1855\n",
      "Epoch [7/10], Step [10000/15000], Loss: 0.0966\n",
      "Epoch [7/10], Step [10250/15000], Loss: 0.0611\n",
      "Epoch [7/10], Step [10500/15000], Loss: 0.1525\n",
      "Epoch [7/10], Step [10750/15000], Loss: 0.1291\n",
      "Epoch [7/10], Step [11000/15000], Loss: 0.1197\n",
      "Epoch [7/10], Step [11250/15000], Loss: 0.1528\n",
      "Epoch [7/10], Step [11500/15000], Loss: 0.1571\n",
      "Epoch [7/10], Step [11750/15000], Loss: 0.1508\n",
      "Epoch [7/10], Step [12000/15000], Loss: 0.1368\n",
      "Epoch [7/10], Step [12250/15000], Loss: 0.1199\n",
      "Epoch [7/10], Step [12500/15000], Loss: 0.1120\n",
      "Epoch [7/10], Step [12750/15000], Loss: 0.1270\n",
      "Epoch [7/10], Step [13000/15000], Loss: 0.0989\n",
      "Epoch [7/10], Step [13250/15000], Loss: 0.1292\n",
      "Epoch [7/10], Step [13500/15000], Loss: 0.1203\n",
      "Epoch [7/10], Step [13750/15000], Loss: 0.1127\n",
      "Epoch [7/10], Step [14000/15000], Loss: 0.0958\n",
      "Epoch [7/10], Step [14250/15000], Loss: 0.1447\n",
      "Epoch [7/10], Step [14500/15000], Loss: 0.1332\n",
      "Epoch [7/10], Step [14750/15000], Loss: 0.1579\n",
      "Epoch [7/10], Step [15000/15000], Loss: 0.1182\n",
      "Epoch [7/10], Average Loss: 0.0000\n",
      "Epoch [8/10], Step [250/15000], Loss: 0.0693\n",
      "Epoch [8/10], Step [500/15000], Loss: 0.1103\n",
      "Epoch [8/10], Step [750/15000], Loss: 0.0743\n",
      "Epoch [8/10], Step [1000/15000], Loss: 0.0847\n",
      "Epoch [8/10], Step [1250/15000], Loss: 0.1115\n",
      "Epoch [8/10], Step [1500/15000], Loss: 0.1552\n",
      "Epoch [8/10], Step [1750/15000], Loss: 0.0937\n",
      "Epoch [8/10], Step [2000/15000], Loss: 0.1131\n",
      "Epoch [8/10], Step [2250/15000], Loss: 0.1266\n",
      "Epoch [8/10], Step [2500/15000], Loss: 0.1195\n",
      "Epoch [8/10], Step [2750/15000], Loss: 0.1310\n",
      "Epoch [8/10], Step [3000/15000], Loss: 0.1255\n",
      "Epoch [8/10], Step [3250/15000], Loss: 0.1127\n",
      "Epoch [8/10], Step [3500/15000], Loss: 0.1063\n",
      "Epoch [8/10], Step [3750/15000], Loss: 0.1021\n",
      "Epoch [8/10], Step [4000/15000], Loss: 0.1132\n",
      "Epoch [8/10], Step [4250/15000], Loss: 0.0910\n",
      "Epoch [8/10], Step [4500/15000], Loss: 0.1433\n",
      "Epoch [8/10], Step [4750/15000], Loss: 0.1374\n",
      "Epoch [8/10], Step [5000/15000], Loss: 0.1257\n",
      "Epoch [8/10], Step [5250/15000], Loss: 0.0985\n",
      "Epoch [8/10], Step [5500/15000], Loss: 0.1324\n",
      "Epoch [8/10], Step [5750/15000], Loss: 0.1094\n",
      "Epoch [8/10], Step [6000/15000], Loss: 0.1024\n",
      "Epoch [8/10], Step [6250/15000], Loss: 0.1384\n",
      "Epoch [8/10], Step [6500/15000], Loss: 0.1095\n",
      "Epoch [8/10], Step [6750/15000], Loss: 0.1645\n",
      "Epoch [8/10], Step [7000/15000], Loss: 0.1070\n",
      "Epoch [8/10], Step [7250/15000], Loss: 0.1071\n",
      "Epoch [8/10], Step [7500/15000], Loss: 0.1120\n",
      "Epoch [8/10], Step [7750/15000], Loss: 0.1043\n",
      "Epoch [8/10], Step [8000/15000], Loss: 0.1106\n",
      "Epoch [8/10], Step [8250/15000], Loss: 0.0703\n",
      "Epoch [8/10], Step [8500/15000], Loss: 0.1048\n",
      "Epoch [8/10], Step [8750/15000], Loss: 0.1474\n",
      "Epoch [8/10], Step [9000/15000], Loss: 0.1357\n",
      "Epoch [8/10], Step [9250/15000], Loss: 0.1230\n",
      "Epoch [8/10], Step [9500/15000], Loss: 0.0849\n",
      "Epoch [8/10], Step [9750/15000], Loss: 0.0946\n",
      "Epoch [8/10], Step [10000/15000], Loss: 0.1218\n",
      "Epoch [8/10], Step [10250/15000], Loss: 0.1198\n",
      "Epoch [8/10], Step [10500/15000], Loss: 0.1701\n",
      "Epoch [8/10], Step [10750/15000], Loss: 0.0896\n",
      "Epoch [8/10], Step [11000/15000], Loss: 0.1719\n",
      "Epoch [8/10], Step [11250/15000], Loss: 0.1108\n",
      "Epoch [8/10], Step [11500/15000], Loss: 0.0942\n",
      "Epoch [8/10], Step [11750/15000], Loss: 0.1410\n",
      "Epoch [8/10], Step [12000/15000], Loss: 0.1359\n",
      "Epoch [8/10], Step [12250/15000], Loss: 0.1358\n",
      "Epoch [8/10], Step [12500/15000], Loss: 0.1477\n",
      "Epoch [8/10], Step [12750/15000], Loss: 0.1617\n",
      "Epoch [8/10], Step [13000/15000], Loss: 0.1076\n",
      "Epoch [8/10], Step [13250/15000], Loss: 0.1145\n",
      "Epoch [8/10], Step [13500/15000], Loss: 0.1284\n",
      "Epoch [8/10], Step [13750/15000], Loss: 0.1055\n",
      "Epoch [8/10], Step [14000/15000], Loss: 0.0810\n",
      "Epoch [8/10], Step [14250/15000], Loss: 0.0647\n",
      "Epoch [8/10], Step [14500/15000], Loss: 0.1487\n",
      "Epoch [8/10], Step [14750/15000], Loss: 0.1382\n",
      "Epoch [8/10], Step [15000/15000], Loss: 0.1280\n",
      "Epoch [8/10], Average Loss: 0.0000\n",
      "Epoch [9/10], Step [250/15000], Loss: 0.0789\n",
      "Epoch [9/10], Step [500/15000], Loss: 0.0939\n",
      "Epoch [9/10], Step [750/15000], Loss: 0.0867\n",
      "Epoch [9/10], Step [1000/15000], Loss: 0.1184\n",
      "Epoch [9/10], Step [1250/15000], Loss: 0.1027\n",
      "Epoch [9/10], Step [1500/15000], Loss: 0.1156\n",
      "Epoch [9/10], Step [1750/15000], Loss: 0.1146\n",
      "Epoch [9/10], Step [2000/15000], Loss: 0.1113\n",
      "Epoch [9/10], Step [2250/15000], Loss: 0.1258\n",
      "Epoch [9/10], Step [2500/15000], Loss: 0.1089\n",
      "Epoch [9/10], Step [2750/15000], Loss: 0.1067\n",
      "Epoch [9/10], Step [3000/15000], Loss: 0.0823\n",
      "Epoch [9/10], Step [3250/15000], Loss: 0.1190\n",
      "Epoch [9/10], Step [3500/15000], Loss: 0.0798\n",
      "Epoch [9/10], Step [3750/15000], Loss: 0.0995\n",
      "Epoch [9/10], Step [4000/15000], Loss: 0.1130\n",
      "Epoch [9/10], Step [4250/15000], Loss: 0.0940\n",
      "Epoch [9/10], Step [4500/15000], Loss: 0.0940\n",
      "Epoch [9/10], Step [4750/15000], Loss: 0.0904\n",
      "Epoch [9/10], Step [5000/15000], Loss: 0.0891\n",
      "Epoch [9/10], Step [5250/15000], Loss: 0.0904\n",
      "Epoch [9/10], Step [5500/15000], Loss: 0.1048\n",
      "Epoch [9/10], Step [5750/15000], Loss: 0.1012\n",
      "Epoch [9/10], Step [6000/15000], Loss: 0.1302\n",
      "Epoch [9/10], Step [6250/15000], Loss: 0.1056\n",
      "Epoch [9/10], Step [6500/15000], Loss: 0.1273\n",
      "Epoch [9/10], Step [6750/15000], Loss: 0.1242\n",
      "Epoch [9/10], Step [7000/15000], Loss: 0.1399\n",
      "Epoch [9/10], Step [7250/15000], Loss: 0.1080\n",
      "Epoch [9/10], Step [7500/15000], Loss: 0.1121\n",
      "Epoch [9/10], Step [7750/15000], Loss: 0.1222\n",
      "Epoch [9/10], Step [8000/15000], Loss: 0.0988\n",
      "Epoch [9/10], Step [8250/15000], Loss: 0.1058\n",
      "Epoch [9/10], Step [8500/15000], Loss: 0.1096\n",
      "Epoch [9/10], Step [8750/15000], Loss: 0.0601\n",
      "Epoch [9/10], Step [9000/15000], Loss: 0.0854\n",
      "Epoch [9/10], Step [9250/15000], Loss: 0.1068\n",
      "Epoch [9/10], Step [9500/15000], Loss: 0.1299\n",
      "Epoch [9/10], Step [9750/15000], Loss: 0.1112\n",
      "Epoch [9/10], Step [10000/15000], Loss: 0.1108\n",
      "Epoch [9/10], Step [10250/15000], Loss: 0.0936\n",
      "Epoch [9/10], Step [10500/15000], Loss: 0.0870\n",
      "Epoch [9/10], Step [10750/15000], Loss: 0.1606\n",
      "Epoch [9/10], Step [11000/15000], Loss: 0.1117\n",
      "Epoch [9/10], Step [11250/15000], Loss: 0.1109\n",
      "Epoch [9/10], Step [11500/15000], Loss: 0.1280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Step [11750/15000], Loss: 0.0884\n",
      "Epoch [9/10], Step [12000/15000], Loss: 0.1142\n",
      "Epoch [9/10], Step [12250/15000], Loss: 0.1065\n",
      "Epoch [9/10], Step [12500/15000], Loss: 0.1143\n",
      "Epoch [9/10], Step [12750/15000], Loss: 0.0770\n",
      "Epoch [9/10], Step [13000/15000], Loss: 0.1318\n",
      "Epoch [9/10], Step [13250/15000], Loss: 0.1127\n",
      "Epoch [9/10], Step [13500/15000], Loss: 0.1672\n",
      "Epoch [9/10], Step [13750/15000], Loss: 0.1511\n",
      "Epoch [9/10], Step [14000/15000], Loss: 0.1376\n",
      "Epoch [9/10], Step [14250/15000], Loss: 0.1550\n",
      "Epoch [9/10], Step [14500/15000], Loss: 0.0804\n",
      "Epoch [9/10], Step [14750/15000], Loss: 0.1232\n",
      "Epoch [9/10], Step [15000/15000], Loss: 0.1242\n",
      "Epoch [9/10], Average Loss: 0.0000\n",
      "Epoch [10/10], Step [250/15000], Loss: 0.0752\n",
      "Epoch [10/10], Step [500/15000], Loss: 0.1019\n",
      "Epoch [10/10], Step [750/15000], Loss: 0.1005\n",
      "Epoch [10/10], Step [1000/15000], Loss: 0.0950\n",
      "Epoch [10/10], Step [1250/15000], Loss: 0.0982\n",
      "Epoch [10/10], Step [1500/15000], Loss: 0.1251\n",
      "Epoch [10/10], Step [1750/15000], Loss: 0.0666\n",
      "Epoch [10/10], Step [2000/15000], Loss: 0.0891\n",
      "Epoch [10/10], Step [2250/15000], Loss: 0.0932\n",
      "Epoch [10/10], Step [2500/15000], Loss: 0.1001\n",
      "Epoch [10/10], Step [2750/15000], Loss: 0.0752\n",
      "Epoch [10/10], Step [3000/15000], Loss: 0.1013\n",
      "Epoch [10/10], Step [3250/15000], Loss: 0.1130\n",
      "Epoch [10/10], Step [3500/15000], Loss: 0.0935\n",
      "Epoch [10/10], Step [3750/15000], Loss: 0.0984\n",
      "Epoch [10/10], Step [4000/15000], Loss: 0.1259\n",
      "Epoch [10/10], Step [4250/15000], Loss: 0.0835\n",
      "Epoch [10/10], Step [4500/15000], Loss: 0.0862\n",
      "Epoch [10/10], Step [4750/15000], Loss: 0.1330\n",
      "Epoch [10/10], Step [5000/15000], Loss: 0.0982\n",
      "Epoch [10/10], Step [5250/15000], Loss: 0.0848\n",
      "Epoch [10/10], Step [5500/15000], Loss: 0.0668\n",
      "Epoch [10/10], Step [5750/15000], Loss: 0.0996\n",
      "Epoch [10/10], Step [6000/15000], Loss: 0.1511\n",
      "Epoch [10/10], Step [6250/15000], Loss: 0.1571\n",
      "Epoch [10/10], Step [6500/15000], Loss: 0.0784\n",
      "Epoch [10/10], Step [6750/15000], Loss: 0.0913\n",
      "Epoch [10/10], Step [7000/15000], Loss: 0.1176\n",
      "Epoch [10/10], Step [7250/15000], Loss: 0.1087\n",
      "Epoch [10/10], Step [7500/15000], Loss: 0.1318\n",
      "Epoch [10/10], Step [7750/15000], Loss: 0.1222\n",
      "Epoch [10/10], Step [8000/15000], Loss: 0.1132\n",
      "Epoch [10/10], Step [8250/15000], Loss: 0.0630\n",
      "Epoch [10/10], Step [8500/15000], Loss: 0.1092\n",
      "Epoch [10/10], Step [8750/15000], Loss: 0.0830\n",
      "Epoch [10/10], Step [9000/15000], Loss: 0.1194\n",
      "Epoch [10/10], Step [9250/15000], Loss: 0.0843\n",
      "Epoch [10/10], Step [9500/15000], Loss: 0.0677\n",
      "Epoch [10/10], Step [9750/15000], Loss: 0.1480\n",
      "Epoch [10/10], Step [10000/15000], Loss: 0.1260\n",
      "Epoch [10/10], Step [10250/15000], Loss: 0.0846\n",
      "Epoch [10/10], Step [10500/15000], Loss: 0.1332\n",
      "Epoch [10/10], Step [10750/15000], Loss: 0.1081\n",
      "Epoch [10/10], Step [11000/15000], Loss: 0.1373\n",
      "Epoch [10/10], Step [11250/15000], Loss: 0.1068\n",
      "Epoch [10/10], Step [11500/15000], Loss: 0.0632\n",
      "Epoch [10/10], Step [11750/15000], Loss: 0.0675\n",
      "Epoch [10/10], Step [12000/15000], Loss: 0.1094\n",
      "Epoch [10/10], Step [12250/15000], Loss: 0.1207\n",
      "Epoch [10/10], Step [12500/15000], Loss: 0.0839\n",
      "Epoch [10/10], Step [12750/15000], Loss: 0.0987\n",
      "Epoch [10/10], Step [13000/15000], Loss: 0.0807\n",
      "Epoch [10/10], Step [13250/15000], Loss: 0.1655\n",
      "Epoch [10/10], Step [13500/15000], Loss: 0.0773\n",
      "Epoch [10/10], Step [13750/15000], Loss: 0.1595\n",
      "Epoch [10/10], Step [14000/15000], Loss: 0.0867\n",
      "Epoch [10/10], Step [14250/15000], Loss: 0.1025\n",
      "Epoch [10/10], Step [14500/15000], Loss: 0.1179\n",
      "Epoch [10/10], Step [14750/15000], Loss: 0.1109\n",
      "Epoch [10/10], Step [15000/15000], Loss: 0.1322\n",
      "Epoch [10/10], Average Loss: 0.0000\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "def train_model(model, trainloader, optimizer, criterion, num_epochs=10, print_every=250):\n",
    "    \"\"\"\n",
    "    This function trains the model and prints the loss every 'print_every' mini-batches.\n",
    "    :param model: The neural network model instance.\n",
    "    :param trainloader: DataLoader for the training set.\n",
    "    :param optimizer: The optimization algorithm.\n",
    "    :param criterion: The loss function.\n",
    "    :param num_epochs: Number of epochs to train the model.\n",
    "    :param print_every: The number of batches after which the training loss is printed.\n",
    "    :return: A list containing the average loss per epoch.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if GPU is available and move the model to GPU\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # To store average loss per epoch\n",
    "    losses = []  \n",
    "    \n",
    "    print(\"Starting Training...\")\n",
    "    \n",
    "    # Iterate over the number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        running_loss = 0.0  # To accumulate loss over mini-batches\n",
    "        \n",
    "        # Enumerate over the DataLoader to get batches of data\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()  \n",
    "            \n",
    "            # Forward pass: Compute predicted y by passing x to the model\n",
    "            outputs = model(inputs) \n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels) \n",
    "            \n",
    "            # Backward pass: Compute gradient of the loss with respect to model parameters\n",
    "            loss.backward() \n",
    "            \n",
    "            # Make a step with the optimizer\n",
    "            optimizer.step() \n",
    "            \n",
    "            # Accumulate loss for the current batch\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Print average loss every 'print_every' mini-batches\n",
    "            if i % print_every == print_every - 1:  \n",
    "                print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(trainloader)}], Loss: {running_loss / print_every:.4f}')\n",
    "                running_loss = 0.0  # Reset running loss\n",
    "        \n",
    "        # Calculate and store the average loss in the current epoch\n",
    "        epoch_loss = running_loss / len(trainloader)\n",
    "        losses.append(epoch_loss)\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Average Loss: {epoch_loss:.4f}')\n",
    "        \n",
    "    print('Finished Training')\n",
    "    return losses\n",
    "\n",
    "\n",
    "# Ensure model, trainloader, optimizer, and criterion are properly initialized before this\n",
    "losses = train_model(model=model, trainloader=trainloader, optimizer=optimizer, criterion=criterion, num_epochs=10, print_every=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training loss (and validation loss/accuracy, if recorded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(num_epochs), losses)\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing your model\n",
    "Using the previously created `DataLoader` for the test set, compute the percentage of correct predictions using the highest probability prediction. \n",
    "\n",
    "If your accuracy is over 90%, great work, but see if you can push a bit further! \n",
    "If your accuracy is under 90%, you'll need to make improvements.\n",
    "Go back and check your model architecture, loss function, and optimizer to make sure they're appropriate for an image classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Testing...\n",
      "Accuracy of the model on the 10000 test images: 95.21%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def test_model(model, testloader):\n",
    "    \"\"\"\n",
    "    This function tests the model on the provided test loader and prints the accuracy.\n",
    "    :param model: The neural network model instance.\n",
    "    :param testloader: DataLoader for the test set.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initializing variables to keep count of correct predictions and total number of samples\n",
    "    correct = 0  \n",
    "    total = 0  \n",
    "    \n",
    "    print(\"Starting Testing...\")\n",
    "    \n",
    "    # Setting the model to evaluation mode. This is important when the model has layers like dropout.\n",
    "    model.eval()\n",
    "    \n",
    "    # torch.no_grad() disables gradient calculation during inference, reducing memory consumption\n",
    "    with torch.no_grad():  \n",
    "        \n",
    "        # Iterate over the test data loader\n",
    "        for data in testloader:\n",
    "            # Move tensors to the appropriate device\n",
    "            images, labels = data[0].to(device), data[1].to(device) \n",
    "            \n",
    "            # Forward pass: Compute predicted y by passing x to the model\n",
    "            outputs = model(images)  \n",
    "            \n",
    "            # Get the index of the highest score as the predicted label\n",
    "            _, predicted = torch.max(outputs.data, 1)  \n",
    "            \n",
    "            # Update the total count\n",
    "            total += labels.size(0)  \n",
    "            \n",
    "            # Update the correct count\n",
    "            correct += (predicted == labels).sum().item()  \n",
    "    \n",
    "    # Calculate the accuracy of the model\n",
    "    accuracy = 100 * correct / total  \n",
    "    \n",
    "    print(f'Accuracy of the model on the {total} test images: {accuracy}%')\n",
    "    \n",
    "    # Setting the model back to training mode\n",
    "    model.train()\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Start testing\n",
    "test_model(model, testloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving your model\n",
    "\n",
    "Once your model is done training, try tweaking your hyperparameters and training again below to improve your accuracy on the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting refining, training, and testing process...\n",
      "Adjusting Learning Rate to 0.0001...\n",
      "Training Model with Learning Rate 0.0001 for 10 epochs...\n",
      "Starting Training...\n",
      "Epoch [1/10], Step [250/15000], Loss: 0.0724\n",
      "Epoch [1/10], Step [500/15000], Loss: 0.0722\n",
      "Epoch [1/10], Step [750/15000], Loss: 0.0709\n",
      "Epoch [1/10], Step [1000/15000], Loss: 0.0620\n",
      "Epoch [1/10], Step [1250/15000], Loss: 0.0298\n",
      "Epoch [1/10], Step [1500/15000], Loss: 0.0570\n",
      "Epoch [1/10], Step [1750/15000], Loss: 0.0583\n",
      "Epoch [1/10], Step [2000/15000], Loss: 0.0589\n",
      "Epoch [1/10], Step [2250/15000], Loss: 0.0507\n",
      "Epoch [1/10], Step [2500/15000], Loss: 0.0518\n",
      "Epoch [1/10], Step [2750/15000], Loss: 0.0404\n",
      "Epoch [1/10], Step [3000/15000], Loss: 0.0624\n",
      "Epoch [1/10], Step [3250/15000], Loss: 0.0671\n",
      "Epoch [1/10], Step [3500/15000], Loss: 0.0466\n",
      "Epoch [1/10], Step [3750/15000], Loss: 0.0435\n",
      "Epoch [1/10], Step [4000/15000], Loss: 0.0505\n",
      "Epoch [1/10], Step [4250/15000], Loss: 0.0644\n",
      "Epoch [1/10], Step [4500/15000], Loss: 0.0405\n",
      "Epoch [1/10], Step [4750/15000], Loss: 0.0544\n",
      "Epoch [1/10], Step [5000/15000], Loss: 0.0332\n",
      "Epoch [1/10], Step [5250/15000], Loss: 0.0369\n",
      "Epoch [1/10], Step [5500/15000], Loss: 0.0475\n",
      "Epoch [1/10], Step [5750/15000], Loss: 0.0383\n",
      "Epoch [1/10], Step [6000/15000], Loss: 0.0456\n",
      "Epoch [1/10], Step [6250/15000], Loss: 0.0415\n",
      "Epoch [1/10], Step [6500/15000], Loss: 0.0412\n",
      "Epoch [1/10], Step [6750/15000], Loss: 0.0746\n",
      "Epoch [1/10], Step [7000/15000], Loss: 0.0701\n",
      "Epoch [1/10], Step [7250/15000], Loss: 0.0409\n",
      "Epoch [1/10], Step [7500/15000], Loss: 0.0425\n",
      "Epoch [1/10], Step [7750/15000], Loss: 0.0625\n",
      "Epoch [1/10], Step [8000/15000], Loss: 0.0624\n",
      "Epoch [1/10], Step [8250/15000], Loss: 0.0408\n",
      "Epoch [1/10], Step [8500/15000], Loss: 0.0531\n",
      "Epoch [1/10], Step [8750/15000], Loss: 0.0477\n",
      "Epoch [1/10], Step [9000/15000], Loss: 0.0537\n",
      "Epoch [1/10], Step [9250/15000], Loss: 0.0463\n",
      "Epoch [1/10], Step [9500/15000], Loss: 0.0491\n",
      "Epoch [1/10], Step [9750/15000], Loss: 0.0561\n",
      "Epoch [1/10], Step [10000/15000], Loss: 0.0774\n",
      "Epoch [1/10], Step [10250/15000], Loss: 0.0593\n",
      "Epoch [1/10], Step [10500/15000], Loss: 0.0574\n",
      "Epoch [1/10], Step [10750/15000], Loss: 0.0507\n",
      "Epoch [1/10], Step [11000/15000], Loss: 0.0483\n",
      "Epoch [1/10], Step [11250/15000], Loss: 0.0333\n",
      "Epoch [1/10], Step [11500/15000], Loss: 0.0420\n",
      "Epoch [1/10], Step [11750/15000], Loss: 0.0506\n",
      "Epoch [1/10], Step [12000/15000], Loss: 0.0371\n",
      "Epoch [1/10], Step [12250/15000], Loss: 0.0586\n",
      "Epoch [1/10], Step [12500/15000], Loss: 0.0264\n",
      "Epoch [1/10], Step [12750/15000], Loss: 0.0438\n",
      "Epoch [1/10], Step [13000/15000], Loss: 0.0743\n",
      "Epoch [1/10], Step [13250/15000], Loss: 0.0626\n",
      "Epoch [1/10], Step [13500/15000], Loss: 0.0240\n",
      "Epoch [1/10], Step [13750/15000], Loss: 0.0384\n",
      "Epoch [1/10], Step [14000/15000], Loss: 0.0579\n",
      "Epoch [1/10], Step [14250/15000], Loss: 0.0687\n",
      "Epoch [1/10], Step [14500/15000], Loss: 0.0696\n",
      "Epoch [1/10], Step [14750/15000], Loss: 0.0554\n",
      "Epoch [1/10], Step [15000/15000], Loss: 0.0400\n",
      "Epoch [1/10], Average Loss: 0.0000\n",
      "Epoch [2/10], Step [250/15000], Loss: 0.0330\n",
      "Epoch [2/10], Step [500/15000], Loss: 0.0538\n",
      "Epoch [2/10], Step [750/15000], Loss: 0.0746\n",
      "Epoch [2/10], Step [1000/15000], Loss: 0.0316\n",
      "Epoch [2/10], Step [1250/15000], Loss: 0.0429\n",
      "Epoch [2/10], Step [1500/15000], Loss: 0.0302\n",
      "Epoch [2/10], Step [1750/15000], Loss: 0.0361\n",
      "Epoch [2/10], Step [2000/15000], Loss: 0.0347\n",
      "Epoch [2/10], Step [2250/15000], Loss: 0.0649\n",
      "Epoch [2/10], Step [2500/15000], Loss: 0.0487\n",
      "Epoch [2/10], Step [2750/15000], Loss: 0.0532\n",
      "Epoch [2/10], Step [3000/15000], Loss: 0.0369\n",
      "Epoch [2/10], Step [3250/15000], Loss: 0.0422\n",
      "Epoch [2/10], Step [3500/15000], Loss: 0.0441\n",
      "Epoch [2/10], Step [3750/15000], Loss: 0.0408\n",
      "Epoch [2/10], Step [4000/15000], Loss: 0.0331\n",
      "Epoch [2/10], Step [4250/15000], Loss: 0.0390\n",
      "Epoch [2/10], Step [4500/15000], Loss: 0.0437\n",
      "Epoch [2/10], Step [4750/15000], Loss: 0.0604\n",
      "Epoch [2/10], Step [5000/15000], Loss: 0.0635\n",
      "Epoch [2/10], Step [5250/15000], Loss: 0.0534\n",
      "Epoch [2/10], Step [5500/15000], Loss: 0.0530\n",
      "Epoch [2/10], Step [5750/15000], Loss: 0.0376\n",
      "Epoch [2/10], Step [6000/15000], Loss: 0.0391\n",
      "Epoch [2/10], Step [6250/15000], Loss: 0.0631\n",
      "Epoch [2/10], Step [6500/15000], Loss: 0.0463\n",
      "Epoch [2/10], Step [6750/15000], Loss: 0.0493\n",
      "Epoch [2/10], Step [7000/15000], Loss: 0.0354\n",
      "Epoch [2/10], Step [7250/15000], Loss: 0.0520\n",
      "Epoch [2/10], Step [7500/15000], Loss: 0.0240\n",
      "Epoch [2/10], Step [7750/15000], Loss: 0.0577\n",
      "Epoch [2/10], Step [8000/15000], Loss: 0.0314\n",
      "Epoch [2/10], Step [8250/15000], Loss: 0.0397\n",
      "Epoch [2/10], Step [8500/15000], Loss: 0.0364\n",
      "Epoch [2/10], Step [8750/15000], Loss: 0.0409\n",
      "Epoch [2/10], Step [9000/15000], Loss: 0.0556\n",
      "Epoch [2/10], Step [9250/15000], Loss: 0.0599\n",
      "Epoch [2/10], Step [9500/15000], Loss: 0.0389\n",
      "Epoch [2/10], Step [9750/15000], Loss: 0.0470\n",
      "Epoch [2/10], Step [10000/15000], Loss: 0.0483\n",
      "Epoch [2/10], Step [10250/15000], Loss: 0.0213\n",
      "Epoch [2/10], Step [10500/15000], Loss: 0.0336\n",
      "Epoch [2/10], Step [10750/15000], Loss: 0.0574\n",
      "Epoch [2/10], Step [11000/15000], Loss: 0.0343\n",
      "Epoch [2/10], Step [11250/15000], Loss: 0.0230\n",
      "Epoch [2/10], Step [11500/15000], Loss: 0.0176\n",
      "Epoch [2/10], Step [11750/15000], Loss: 0.0412\n",
      "Epoch [2/10], Step [12000/15000], Loss: 0.0364\n",
      "Epoch [2/10], Step [12250/15000], Loss: 0.0244\n",
      "Epoch [2/10], Step [12500/15000], Loss: 0.0406\n",
      "Epoch [2/10], Step [12750/15000], Loss: 0.0696\n",
      "Epoch [2/10], Step [13000/15000], Loss: 0.0366\n",
      "Epoch [2/10], Step [13250/15000], Loss: 0.0410\n",
      "Epoch [2/10], Step [13500/15000], Loss: 0.0442\n",
      "Epoch [2/10], Step [13750/15000], Loss: 0.0570\n",
      "Epoch [2/10], Step [14000/15000], Loss: 0.0501\n",
      "Epoch [2/10], Step [14250/15000], Loss: 0.0631\n",
      "Epoch [2/10], Step [14500/15000], Loss: 0.0290\n",
      "Epoch [2/10], Step [14750/15000], Loss: 0.0380\n",
      "Epoch [2/10], Step [15000/15000], Loss: 0.0460\n",
      "Epoch [2/10], Average Loss: 0.0000\n",
      "Epoch [3/10], Step [250/15000], Loss: 0.0148\n",
      "Epoch [3/10], Step [500/15000], Loss: 0.0243\n",
      "Epoch [3/10], Step [750/15000], Loss: 0.0484\n",
      "Epoch [3/10], Step [1000/15000], Loss: 0.0493\n",
      "Epoch [3/10], Step [1250/15000], Loss: 0.0374\n",
      "Epoch [3/10], Step [1500/15000], Loss: 0.0368\n",
      "Epoch [3/10], Step [1750/15000], Loss: 0.0329\n",
      "Epoch [3/10], Step [2000/15000], Loss: 0.0439\n",
      "Epoch [3/10], Step [2250/15000], Loss: 0.0288\n",
      "Epoch [3/10], Step [2500/15000], Loss: 0.0463\n",
      "Epoch [3/10], Step [2750/15000], Loss: 0.0209\n",
      "Epoch [3/10], Step [3000/15000], Loss: 0.0419\n",
      "Epoch [3/10], Step [3250/15000], Loss: 0.0164\n",
      "Epoch [3/10], Step [3500/15000], Loss: 0.0403\n",
      "Epoch [3/10], Step [3750/15000], Loss: 0.0627\n",
      "Epoch [3/10], Step [4000/15000], Loss: 0.0425\n",
      "Epoch [3/10], Step [4250/15000], Loss: 0.0408\n",
      "Epoch [3/10], Step [4500/15000], Loss: 0.0427\n",
      "Epoch [3/10], Step [4750/15000], Loss: 0.0253\n",
      "Epoch [3/10], Step [5000/15000], Loss: 0.0562\n",
      "Epoch [3/10], Step [5250/15000], Loss: 0.0393\n",
      "Epoch [3/10], Step [5500/15000], Loss: 0.0341\n",
      "Epoch [3/10], Step [5750/15000], Loss: 0.0387\n",
      "Epoch [3/10], Step [6000/15000], Loss: 0.0430\n",
      "Epoch [3/10], Step [6250/15000], Loss: 0.0455\n",
      "Epoch [3/10], Step [6500/15000], Loss: 0.0510\n",
      "Epoch [3/10], Step [6750/15000], Loss: 0.0529\n",
      "Epoch [3/10], Step [7000/15000], Loss: 0.0313\n",
      "Epoch [3/10], Step [7250/15000], Loss: 0.0488\n",
      "Epoch [3/10], Step [7500/15000], Loss: 0.0547\n",
      "Epoch [3/10], Step [7750/15000], Loss: 0.0214\n",
      "Epoch [3/10], Step [8000/15000], Loss: 0.0386\n",
      "Epoch [3/10], Step [8250/15000], Loss: 0.0400\n",
      "Epoch [3/10], Step [8500/15000], Loss: 0.0306\n",
      "Epoch [3/10], Step [8750/15000], Loss: 0.0458\n",
      "Epoch [3/10], Step [9000/15000], Loss: 0.0303\n",
      "Epoch [3/10], Step [9250/15000], Loss: 0.0474\n",
      "Epoch [3/10], Step [9500/15000], Loss: 0.0233\n",
      "Epoch [3/10], Step [9750/15000], Loss: 0.0332\n",
      "Epoch [3/10], Step [10000/15000], Loss: 0.0475\n",
      "Epoch [3/10], Step [10250/15000], Loss: 0.0350\n",
      "Epoch [3/10], Step [10500/15000], Loss: 0.0266\n",
      "Epoch [3/10], Step [10750/15000], Loss: 0.0571\n",
      "Epoch [3/10], Step [11000/15000], Loss: 0.0400\n",
      "Epoch [3/10], Step [11250/15000], Loss: 0.0437\n",
      "Epoch [3/10], Step [11500/15000], Loss: 0.0356\n",
      "Epoch [3/10], Step [11750/15000], Loss: 0.0202\n",
      "Epoch [3/10], Step [12000/15000], Loss: 0.0759\n",
      "Epoch [3/10], Step [12250/15000], Loss: 0.0402\n",
      "Epoch [3/10], Step [12500/15000], Loss: 0.0584\n",
      "Epoch [3/10], Step [12750/15000], Loss: 0.0318\n",
      "Epoch [3/10], Step [13000/15000], Loss: 0.0370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Step [13250/15000], Loss: 0.0540\n",
      "Epoch [3/10], Step [13500/15000], Loss: 0.0374\n",
      "Epoch [3/10], Step [13750/15000], Loss: 0.0444\n",
      "Epoch [3/10], Step [14000/15000], Loss: 0.0297\n",
      "Epoch [3/10], Step [14250/15000], Loss: 0.0728\n",
      "Epoch [3/10], Step [14500/15000], Loss: 0.0564\n",
      "Epoch [3/10], Step [14750/15000], Loss: 0.0294\n",
      "Epoch [3/10], Step [15000/15000], Loss: 0.0529\n",
      "Epoch [3/10], Average Loss: 0.0000\n",
      "Epoch [4/10], Step [250/15000], Loss: 0.0673\n",
      "Epoch [4/10], Step [500/15000], Loss: 0.0259\n",
      "Epoch [4/10], Step [750/15000], Loss: 0.0391\n",
      "Epoch [4/10], Step [1000/15000], Loss: 0.0595\n",
      "Epoch [4/10], Step [1250/15000], Loss: 0.0247\n",
      "Epoch [4/10], Step [1500/15000], Loss: 0.0297\n",
      "Epoch [4/10], Step [1750/15000], Loss: 0.0362\n",
      "Epoch [4/10], Step [2000/15000], Loss: 0.0370\n",
      "Epoch [4/10], Step [2250/15000], Loss: 0.0155\n",
      "Epoch [4/10], Step [2500/15000], Loss: 0.0492\n",
      "Epoch [4/10], Step [2750/15000], Loss: 0.0371\n",
      "Epoch [4/10], Step [3000/15000], Loss: 0.0509\n",
      "Epoch [4/10], Step [3250/15000], Loss: 0.0350\n",
      "Epoch [4/10], Step [3500/15000], Loss: 0.0297\n",
      "Epoch [4/10], Step [3750/15000], Loss: 0.0377\n",
      "Epoch [4/10], Step [4000/15000], Loss: 0.0412\n",
      "Epoch [4/10], Step [4250/15000], Loss: 0.0341\n",
      "Epoch [4/10], Step [4500/15000], Loss: 0.0397\n",
      "Epoch [4/10], Step [4750/15000], Loss: 0.0261\n",
      "Epoch [4/10], Step [5000/15000], Loss: 0.0415\n",
      "Epoch [4/10], Step [5250/15000], Loss: 0.0481\n",
      "Epoch [4/10], Step [5500/15000], Loss: 0.0905\n",
      "Epoch [4/10], Step [5750/15000], Loss: 0.0399\n",
      "Epoch [4/10], Step [6000/15000], Loss: 0.0284\n",
      "Epoch [4/10], Step [6250/15000], Loss: 0.0372\n",
      "Epoch [4/10], Step [6500/15000], Loss: 0.0451\n",
      "Epoch [4/10], Step [6750/15000], Loss: 0.0215\n",
      "Epoch [4/10], Step [7000/15000], Loss: 0.0362\n",
      "Epoch [4/10], Step [7250/15000], Loss: 0.0337\n",
      "Epoch [4/10], Step [7500/15000], Loss: 0.0408\n",
      "Epoch [4/10], Step [7750/15000], Loss: 0.0242\n",
      "Epoch [4/10], Step [8000/15000], Loss: 0.0454\n",
      "Epoch [4/10], Step [8250/15000], Loss: 0.0358\n",
      "Epoch [4/10], Step [8500/15000], Loss: 0.0357\n",
      "Epoch [4/10], Step [8750/15000], Loss: 0.0423\n",
      "Epoch [4/10], Step [9000/15000], Loss: 0.0275\n",
      "Epoch [4/10], Step [9250/15000], Loss: 0.0463\n",
      "Epoch [4/10], Step [9500/15000], Loss: 0.0403\n",
      "Epoch [4/10], Step [9750/15000], Loss: 0.0464\n",
      "Epoch [4/10], Step [10000/15000], Loss: 0.0433\n",
      "Epoch [4/10], Step [10250/15000], Loss: 0.0460\n",
      "Epoch [4/10], Step [10500/15000], Loss: 0.0161\n",
      "Epoch [4/10], Step [10750/15000], Loss: 0.0308\n",
      "Epoch [4/10], Step [11000/15000], Loss: 0.0404\n",
      "Epoch [4/10], Step [11250/15000], Loss: 0.0516\n",
      "Epoch [4/10], Step [11500/15000], Loss: 0.0380\n",
      "Epoch [4/10], Step [11750/15000], Loss: 0.0411\n",
      "Epoch [4/10], Step [12000/15000], Loss: 0.0248\n",
      "Epoch [4/10], Step [12250/15000], Loss: 0.0574\n",
      "Epoch [4/10], Step [12500/15000], Loss: 0.0417\n",
      "Epoch [4/10], Step [12750/15000], Loss: 0.0377\n",
      "Epoch [4/10], Step [13000/15000], Loss: 0.0350\n",
      "Epoch [4/10], Step [13250/15000], Loss: 0.0454\n",
      "Epoch [4/10], Step [13500/15000], Loss: 0.0281\n",
      "Epoch [4/10], Step [13750/15000], Loss: 0.0211\n",
      "Epoch [4/10], Step [14000/15000], Loss: 0.0345\n",
      "Epoch [4/10], Step [14250/15000], Loss: 0.0261\n",
      "Epoch [4/10], Step [14500/15000], Loss: 0.0284\n",
      "Epoch [4/10], Step [14750/15000], Loss: 0.0471\n",
      "Epoch [4/10], Step [15000/15000], Loss: 0.0266\n",
      "Epoch [4/10], Average Loss: 0.0000\n",
      "Epoch [5/10], Step [250/15000], Loss: 0.0320\n",
      "Epoch [5/10], Step [500/15000], Loss: 0.0240\n",
      "Epoch [5/10], Step [750/15000], Loss: 0.0354\n",
      "Epoch [5/10], Step [1000/15000], Loss: 0.0510\n",
      "Epoch [5/10], Step [1250/15000], Loss: 0.0243\n",
      "Epoch [5/10], Step [1500/15000], Loss: 0.0613\n",
      "Epoch [5/10], Step [1750/15000], Loss: 0.0253\n",
      "Epoch [5/10], Step [2000/15000], Loss: 0.0397\n",
      "Epoch [5/10], Step [2250/15000], Loss: 0.0428\n",
      "Epoch [5/10], Step [2500/15000], Loss: 0.0410\n",
      "Epoch [5/10], Step [2750/15000], Loss: 0.0404\n",
      "Epoch [5/10], Step [3000/15000], Loss: 0.0366\n",
      "Epoch [5/10], Step [3250/15000], Loss: 0.0321\n",
      "Epoch [5/10], Step [3500/15000], Loss: 0.0315\n",
      "Epoch [5/10], Step [3750/15000], Loss: 0.0349\n",
      "Epoch [5/10], Step [4000/15000], Loss: 0.0320\n",
      "Epoch [5/10], Step [4250/15000], Loss: 0.0660\n",
      "Epoch [5/10], Step [4500/15000], Loss: 0.0278\n",
      "Epoch [5/10], Step [4750/15000], Loss: 0.0269\n",
      "Epoch [5/10], Step [5000/15000], Loss: 0.0287\n",
      "Epoch [5/10], Step [5250/15000], Loss: 0.0358\n",
      "Epoch [5/10], Step [5500/15000], Loss: 0.0310\n",
      "Epoch [5/10], Step [5750/15000], Loss: 0.0620\n",
      "Epoch [5/10], Step [6000/15000], Loss: 0.0245\n",
      "Epoch [5/10], Step [6250/15000], Loss: 0.0257\n",
      "Epoch [5/10], Step [6500/15000], Loss: 0.0275\n",
      "Epoch [5/10], Step [6750/15000], Loss: 0.0317\n",
      "Epoch [5/10], Step [7000/15000], Loss: 0.0369\n",
      "Epoch [5/10], Step [7250/15000], Loss: 0.0285\n",
      "Epoch [5/10], Step [7500/15000], Loss: 0.0357\n",
      "Epoch [5/10], Step [7750/15000], Loss: 0.0370\n",
      "Epoch [5/10], Step [8000/15000], Loss: 0.0233\n",
      "Epoch [5/10], Step [8250/15000], Loss: 0.0344\n",
      "Epoch [5/10], Step [8500/15000], Loss: 0.0541\n",
      "Epoch [5/10], Step [8750/15000], Loss: 0.0370\n",
      "Epoch [5/10], Step [9000/15000], Loss: 0.0388\n",
      "Epoch [5/10], Step [9250/15000], Loss: 0.0413\n",
      "Epoch [5/10], Step [9500/15000], Loss: 0.0295\n",
      "Epoch [5/10], Step [9750/15000], Loss: 0.0319\n",
      "Epoch [5/10], Step [10000/15000], Loss: 0.0669\n",
      "Epoch [5/10], Step [10250/15000], Loss: 0.0272\n",
      "Epoch [5/10], Step [10500/15000], Loss: 0.0218\n",
      "Epoch [5/10], Step [10750/15000], Loss: 0.0430\n",
      "Epoch [5/10], Step [11000/15000], Loss: 0.0290\n",
      "Epoch [5/10], Step [11250/15000], Loss: 0.0521\n",
      "Epoch [5/10], Step [11500/15000], Loss: 0.0322\n",
      "Epoch [5/10], Step [11750/15000], Loss: 0.0343\n",
      "Epoch [5/10], Step [12000/15000], Loss: 0.0348\n",
      "Epoch [5/10], Step [12250/15000], Loss: 0.0301\n",
      "Epoch [5/10], Step [12500/15000], Loss: 0.0204\n",
      "Epoch [5/10], Step [12750/15000], Loss: 0.0303\n",
      "Epoch [5/10], Step [13000/15000], Loss: 0.0398\n",
      "Epoch [5/10], Step [13250/15000], Loss: 0.0275\n",
      "Epoch [5/10], Step [13500/15000], Loss: 0.0401\n",
      "Epoch [5/10], Step [13750/15000], Loss: 0.0492\n",
      "Epoch [5/10], Step [14000/15000], Loss: 0.0429\n",
      "Epoch [5/10], Step [14250/15000], Loss: 0.0362\n",
      "Epoch [5/10], Step [14500/15000], Loss: 0.0480\n",
      "Epoch [5/10], Step [14750/15000], Loss: 0.0286\n",
      "Epoch [5/10], Step [15000/15000], Loss: 0.0311\n",
      "Epoch [5/10], Average Loss: 0.0000\n",
      "Epoch [6/10], Step [250/15000], Loss: 0.0387\n",
      "Epoch [6/10], Step [500/15000], Loss: 0.0502\n",
      "Epoch [6/10], Step [750/15000], Loss: 0.0342\n",
      "Epoch [6/10], Step [1000/15000], Loss: 0.0242\n",
      "Epoch [6/10], Step [1250/15000], Loss: 0.0453\n",
      "Epoch [6/10], Step [1500/15000], Loss: 0.0416\n",
      "Epoch [6/10], Step [1750/15000], Loss: 0.0283\n",
      "Epoch [6/10], Step [2000/15000], Loss: 0.0343\n",
      "Epoch [6/10], Step [2250/15000], Loss: 0.0581\n",
      "Epoch [6/10], Step [2500/15000], Loss: 0.0382\n",
      "Epoch [6/10], Step [2750/15000], Loss: 0.0295\n",
      "Epoch [6/10], Step [3000/15000], Loss: 0.0387\n",
      "Epoch [6/10], Step [3250/15000], Loss: 0.0258\n",
      "Epoch [6/10], Step [3500/15000], Loss: 0.0250\n",
      "Epoch [6/10], Step [3750/15000], Loss: 0.0369\n",
      "Epoch [6/10], Step [4000/15000], Loss: 0.0370\n",
      "Epoch [6/10], Step [4250/15000], Loss: 0.0506\n",
      "Epoch [6/10], Step [4500/15000], Loss: 0.0158\n",
      "Epoch [6/10], Step [4750/15000], Loss: 0.0227\n",
      "Epoch [6/10], Step [5000/15000], Loss: 0.0290\n",
      "Epoch [6/10], Step [5250/15000], Loss: 0.0468\n",
      "Epoch [6/10], Step [5500/15000], Loss: 0.0344\n",
      "Epoch [6/10], Step [5750/15000], Loss: 0.0271\n",
      "Epoch [6/10], Step [6000/15000], Loss: 0.0339\n",
      "Epoch [6/10], Step [6250/15000], Loss: 0.0262\n",
      "Epoch [6/10], Step [6500/15000], Loss: 0.0248\n",
      "Epoch [6/10], Step [6750/15000], Loss: 0.0259\n",
      "Epoch [6/10], Step [7000/15000], Loss: 0.0296\n",
      "Epoch [6/10], Step [7250/15000], Loss: 0.0303\n",
      "Epoch [6/10], Step [7500/15000], Loss: 0.0397\n",
      "Epoch [6/10], Step [7750/15000], Loss: 0.0438\n",
      "Epoch [6/10], Step [8000/15000], Loss: 0.0255\n",
      "Epoch [6/10], Step [8250/15000], Loss: 0.0278\n",
      "Epoch [6/10], Step [8500/15000], Loss: 0.0210\n",
      "Epoch [6/10], Step [8750/15000], Loss: 0.0404\n",
      "Epoch [6/10], Step [9000/15000], Loss: 0.0347\n",
      "Epoch [6/10], Step [9250/15000], Loss: 0.0417\n",
      "Epoch [6/10], Step [9500/15000], Loss: 0.0261\n",
      "Epoch [6/10], Step [9750/15000], Loss: 0.0363\n",
      "Epoch [6/10], Step [10000/15000], Loss: 0.0370\n",
      "Epoch [6/10], Step [10250/15000], Loss: 0.0396\n",
      "Epoch [6/10], Step [10500/15000], Loss: 0.0388\n",
      "Epoch [6/10], Step [10750/15000], Loss: 0.0339\n",
      "Epoch [6/10], Step [11000/15000], Loss: 0.0402\n",
      "Epoch [6/10], Step [11250/15000], Loss: 0.0389\n",
      "Epoch [6/10], Step [11500/15000], Loss: 0.0227\n",
      "Epoch [6/10], Step [11750/15000], Loss: 0.0266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Step [12000/15000], Loss: 0.0365\n",
      "Epoch [6/10], Step [12250/15000], Loss: 0.0423\n",
      "Epoch [6/10], Step [12500/15000], Loss: 0.0244\n",
      "Epoch [6/10], Step [12750/15000], Loss: 0.0327\n",
      "Epoch [6/10], Step [13000/15000], Loss: 0.0302\n",
      "Epoch [6/10], Step [13250/15000], Loss: 0.0876\n",
      "Epoch [6/10], Step [13500/15000], Loss: 0.0279\n",
      "Epoch [6/10], Step [13750/15000], Loss: 0.0385\n",
      "Epoch [6/10], Step [14000/15000], Loss: 0.0336\n",
      "Epoch [6/10], Step [14250/15000], Loss: 0.0438\n",
      "Epoch [6/10], Step [14500/15000], Loss: 0.0268\n",
      "Epoch [6/10], Step [14750/15000], Loss: 0.0292\n",
      "Epoch [6/10], Step [15000/15000], Loss: 0.0356\n",
      "Epoch [6/10], Average Loss: 0.0000\n",
      "Epoch [7/10], Step [250/15000], Loss: 0.0288\n",
      "Epoch [7/10], Step [500/15000], Loss: 0.0289\n",
      "Epoch [7/10], Step [750/15000], Loss: 0.0359\n",
      "Epoch [7/10], Step [1000/15000], Loss: 0.0488\n",
      "Epoch [7/10], Step [1250/15000], Loss: 0.0272\n",
      "Epoch [7/10], Step [1500/15000], Loss: 0.0209\n",
      "Epoch [7/10], Step [1750/15000], Loss: 0.0437\n",
      "Epoch [7/10], Step [2000/15000], Loss: 0.0128\n",
      "Epoch [7/10], Step [2250/15000], Loss: 0.0610\n",
      "Epoch [7/10], Step [2500/15000], Loss: 0.0451\n",
      "Epoch [7/10], Step [2750/15000], Loss: 0.0215\n",
      "Epoch [7/10], Step [3000/15000], Loss: 0.0286\n",
      "Epoch [7/10], Step [3250/15000], Loss: 0.0328\n",
      "Epoch [7/10], Step [3500/15000], Loss: 0.0387\n",
      "Epoch [7/10], Step [3750/15000], Loss: 0.0449\n",
      "Epoch [7/10], Step [4000/15000], Loss: 0.0167\n",
      "Epoch [7/10], Step [4250/15000], Loss: 0.0402\n",
      "Epoch [7/10], Step [4500/15000], Loss: 0.0273\n",
      "Epoch [7/10], Step [4750/15000], Loss: 0.0185\n",
      "Epoch [7/10], Step [5000/15000], Loss: 0.0412\n",
      "Epoch [7/10], Step [5250/15000], Loss: 0.0246\n",
      "Epoch [7/10], Step [5500/15000], Loss: 0.0231\n",
      "Epoch [7/10], Step [5750/15000], Loss: 0.0229\n",
      "Epoch [7/10], Step [6000/15000], Loss: 0.0378\n",
      "Epoch [7/10], Step [6250/15000], Loss: 0.0577\n",
      "Epoch [7/10], Step [6500/15000], Loss: 0.0378\n",
      "Epoch [7/10], Step [6750/15000], Loss: 0.0432\n",
      "Epoch [7/10], Step [7000/15000], Loss: 0.0451\n",
      "Epoch [7/10], Step [7250/15000], Loss: 0.0205\n",
      "Epoch [7/10], Step [7500/15000], Loss: 0.0707\n",
      "Epoch [7/10], Step [7750/15000], Loss: 0.0442\n",
      "Epoch [7/10], Step [8000/15000], Loss: 0.0373\n",
      "Epoch [7/10], Step [8250/15000], Loss: 0.0312\n",
      "Epoch [7/10], Step [8500/15000], Loss: 0.0198\n",
      "Epoch [7/10], Step [8750/15000], Loss: 0.0317\n",
      "Epoch [7/10], Step [9000/15000], Loss: 0.0340\n",
      "Epoch [7/10], Step [9250/15000], Loss: 0.0333\n",
      "Epoch [7/10], Step [9500/15000], Loss: 0.0152\n",
      "Epoch [7/10], Step [9750/15000], Loss: 0.0306\n",
      "Epoch [7/10], Step [10000/15000], Loss: 0.0299\n",
      "Epoch [7/10], Step [10250/15000], Loss: 0.0395\n",
      "Epoch [7/10], Step [10500/15000], Loss: 0.0256\n",
      "Epoch [7/10], Step [10750/15000], Loss: 0.0303\n",
      "Epoch [7/10], Step [11000/15000], Loss: 0.0264\n",
      "Epoch [7/10], Step [11250/15000], Loss: 0.0339\n",
      "Epoch [7/10], Step [11500/15000], Loss: 0.0381\n",
      "Epoch [7/10], Step [11750/15000], Loss: 0.0344\n",
      "Epoch [7/10], Step [12000/15000], Loss: 0.0470\n",
      "Epoch [7/10], Step [12250/15000], Loss: 0.0215\n",
      "Epoch [7/10], Step [12500/15000], Loss: 0.0393\n",
      "Epoch [7/10], Step [12750/15000], Loss: 0.0341\n",
      "Epoch [7/10], Step [13000/15000], Loss: 0.0328\n",
      "Epoch [7/10], Step [13250/15000], Loss: 0.0285\n",
      "Epoch [7/10], Step [13500/15000], Loss: 0.0275\n",
      "Epoch [7/10], Step [13750/15000], Loss: 0.0376\n",
      "Epoch [7/10], Step [14000/15000], Loss: 0.0356\n",
      "Epoch [7/10], Step [14250/15000], Loss: 0.0407\n",
      "Epoch [7/10], Step [14500/15000], Loss: 0.0454\n",
      "Epoch [7/10], Step [14750/15000], Loss: 0.0238\n",
      "Epoch [7/10], Step [15000/15000], Loss: 0.0267\n",
      "Epoch [7/10], Average Loss: 0.0000\n",
      "Epoch [8/10], Step [250/15000], Loss: 0.0293\n",
      "Epoch [8/10], Step [500/15000], Loss: 0.0374\n",
      "Epoch [8/10], Step [750/15000], Loss: 0.0329\n",
      "Epoch [8/10], Step [1000/15000], Loss: 0.0415\n",
      "Epoch [8/10], Step [1250/15000], Loss: 0.0257\n",
      "Epoch [8/10], Step [1500/15000], Loss: 0.0418\n",
      "Epoch [8/10], Step [1750/15000], Loss: 0.0170\n",
      "Epoch [8/10], Step [2000/15000], Loss: 0.0246\n",
      "Epoch [8/10], Step [2250/15000], Loss: 0.0537\n",
      "Epoch [8/10], Step [2500/15000], Loss: 0.0323\n",
      "Epoch [8/10], Step [2750/15000], Loss: 0.0282\n",
      "Epoch [8/10], Step [3000/15000], Loss: 0.0192\n",
      "Epoch [8/10], Step [3250/15000], Loss: 0.0356\n",
      "Epoch [8/10], Step [3500/15000], Loss: 0.0283\n",
      "Epoch [8/10], Step [3750/15000], Loss: 0.0251\n",
      "Epoch [8/10], Step [4000/15000], Loss: 0.0403\n",
      "Epoch [8/10], Step [4250/15000], Loss: 0.0317\n",
      "Epoch [8/10], Step [4500/15000], Loss: 0.0284\n",
      "Epoch [8/10], Step [4750/15000], Loss: 0.0266\n",
      "Epoch [8/10], Step [5000/15000], Loss: 0.0275\n",
      "Epoch [8/10], Step [5250/15000], Loss: 0.0353\n",
      "Epoch [8/10], Step [5500/15000], Loss: 0.0438\n",
      "Epoch [8/10], Step [5750/15000], Loss: 0.0185\n",
      "Epoch [8/10], Step [6000/15000], Loss: 0.0488\n",
      "Epoch [8/10], Step [6250/15000], Loss: 0.0241\n",
      "Epoch [8/10], Step [6500/15000], Loss: 0.0256\n",
      "Epoch [8/10], Step [6750/15000], Loss: 0.0395\n",
      "Epoch [8/10], Step [7000/15000], Loss: 0.0439\n",
      "Epoch [8/10], Step [7250/15000], Loss: 0.0223\n",
      "Epoch [8/10], Step [7500/15000], Loss: 0.0201\n",
      "Epoch [8/10], Step [7750/15000], Loss: 0.0336\n",
      "Epoch [8/10], Step [8000/15000], Loss: 0.0424\n",
      "Epoch [8/10], Step [8250/15000], Loss: 0.0297\n",
      "Epoch [8/10], Step [8500/15000], Loss: 0.0337\n",
      "Epoch [8/10], Step [8750/15000], Loss: 0.0549\n",
      "Epoch [8/10], Step [9000/15000], Loss: 0.0398\n",
      "Epoch [8/10], Step [9250/15000], Loss: 0.0284\n",
      "Epoch [8/10], Step [9500/15000], Loss: 0.0277\n",
      "Epoch [8/10], Step [9750/15000], Loss: 0.0293\n",
      "Epoch [8/10], Step [10000/15000], Loss: 0.0281\n",
      "Epoch [8/10], Step [10250/15000], Loss: 0.0389\n",
      "Epoch [8/10], Step [10500/15000], Loss: 0.0346\n",
      "Epoch [8/10], Step [10750/15000], Loss: 0.0358\n",
      "Epoch [8/10], Step [11000/15000], Loss: 0.0175\n",
      "Epoch [8/10], Step [11250/15000], Loss: 0.0314\n",
      "Epoch [8/10], Step [11500/15000], Loss: 0.0441\n",
      "Epoch [8/10], Step [11750/15000], Loss: 0.0398\n",
      "Epoch [8/10], Step [12000/15000], Loss: 0.0256\n",
      "Epoch [8/10], Step [12250/15000], Loss: 0.0267\n",
      "Epoch [8/10], Step [12500/15000], Loss: 0.0209\n",
      "Epoch [8/10], Step [12750/15000], Loss: 0.0435\n",
      "Epoch [8/10], Step [13000/15000], Loss: 0.0364\n",
      "Epoch [8/10], Step [13250/15000], Loss: 0.0190\n",
      "Epoch [8/10], Step [13500/15000], Loss: 0.0302\n",
      "Epoch [8/10], Step [13750/15000], Loss: 0.0239\n",
      "Epoch [8/10], Step [14000/15000], Loss: 0.0240\n",
      "Epoch [8/10], Step [14250/15000], Loss: 0.0423\n",
      "Epoch [8/10], Step [14500/15000], Loss: 0.0434\n",
      "Epoch [8/10], Step [14750/15000], Loss: 0.0306\n",
      "Epoch [8/10], Step [15000/15000], Loss: 0.0286\n",
      "Epoch [8/10], Average Loss: 0.0000\n",
      "Epoch [9/10], Step [250/15000], Loss: 0.0534\n",
      "Epoch [9/10], Step [500/15000], Loss: 0.0554\n",
      "Epoch [9/10], Step [750/15000], Loss: 0.0403\n",
      "Epoch [9/10], Step [1000/15000], Loss: 0.0285\n",
      "Epoch [9/10], Step [1250/15000], Loss: 0.0467\n",
      "Epoch [9/10], Step [1500/15000], Loss: 0.0411\n",
      "Epoch [9/10], Step [1750/15000], Loss: 0.0308\n",
      "Epoch [9/10], Step [2000/15000], Loss: 0.0168\n",
      "Epoch [9/10], Step [2250/15000], Loss: 0.0456\n",
      "Epoch [9/10], Step [2500/15000], Loss: 0.0303\n",
      "Epoch [9/10], Step [2750/15000], Loss: 0.0167\n",
      "Epoch [9/10], Step [3000/15000], Loss: 0.0245\n",
      "Epoch [9/10], Step [3250/15000], Loss: 0.0225\n",
      "Epoch [9/10], Step [3500/15000], Loss: 0.0158\n",
      "Epoch [9/10], Step [3750/15000], Loss: 0.0454\n",
      "Epoch [9/10], Step [4000/15000], Loss: 0.0238\n",
      "Epoch [9/10], Step [4250/15000], Loss: 0.0168\n",
      "Epoch [9/10], Step [4500/15000], Loss: 0.0265\n",
      "Epoch [9/10], Step [4750/15000], Loss: 0.0270\n",
      "Epoch [9/10], Step [5000/15000], Loss: 0.0317\n",
      "Epoch [9/10], Step [5250/15000], Loss: 0.0487\n",
      "Epoch [9/10], Step [5500/15000], Loss: 0.0292\n",
      "Epoch [9/10], Step [5750/15000], Loss: 0.0242\n",
      "Epoch [9/10], Step [6000/15000], Loss: 0.0231\n",
      "Epoch [9/10], Step [6250/15000], Loss: 0.0275\n",
      "Epoch [9/10], Step [6500/15000], Loss: 0.0255\n",
      "Epoch [9/10], Step [6750/15000], Loss: 0.0248\n",
      "Epoch [9/10], Step [7000/15000], Loss: 0.0355\n",
      "Epoch [9/10], Step [7250/15000], Loss: 0.0210\n",
      "Epoch [9/10], Step [7500/15000], Loss: 0.0229\n",
      "Epoch [9/10], Step [7750/15000], Loss: 0.0409\n",
      "Epoch [9/10], Step [8000/15000], Loss: 0.0252\n",
      "Epoch [9/10], Step [8250/15000], Loss: 0.0349\n",
      "Epoch [9/10], Step [8500/15000], Loss: 0.0285\n",
      "Epoch [9/10], Step [8750/15000], Loss: 0.0208\n",
      "Epoch [9/10], Step [9000/15000], Loss: 0.0493\n",
      "Epoch [9/10], Step [9250/15000], Loss: 0.0298\n",
      "Epoch [9/10], Step [9500/15000], Loss: 0.0171\n",
      "Epoch [9/10], Step [9750/15000], Loss: 0.0273\n",
      "Epoch [9/10], Step [10000/15000], Loss: 0.0522\n",
      "Epoch [9/10], Step [10250/15000], Loss: 0.0356\n",
      "Epoch [9/10], Step [10500/15000], Loss: 0.0203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Step [10750/15000], Loss: 0.0267\n",
      "Epoch [9/10], Step [11000/15000], Loss: 0.0245\n",
      "Epoch [9/10], Step [11250/15000], Loss: 0.0443\n",
      "Epoch [9/10], Step [11500/15000], Loss: 0.0312\n",
      "Epoch [9/10], Step [11750/15000], Loss: 0.0555\n",
      "Epoch [9/10], Step [12000/15000], Loss: 0.0310\n",
      "Epoch [9/10], Step [12250/15000], Loss: 0.0199\n",
      "Epoch [9/10], Step [12500/15000], Loss: 0.0359\n",
      "Epoch [9/10], Step [12750/15000], Loss: 0.0308\n",
      "Epoch [9/10], Step [13000/15000], Loss: 0.0501\n",
      "Epoch [9/10], Step [13250/15000], Loss: 0.0254\n",
      "Epoch [9/10], Step [13500/15000], Loss: 0.0274\n",
      "Epoch [9/10], Step [13750/15000], Loss: 0.0265\n",
      "Epoch [9/10], Step [14000/15000], Loss: 0.0220\n",
      "Epoch [9/10], Step [14250/15000], Loss: 0.0240\n",
      "Epoch [9/10], Step [14500/15000], Loss: 0.0403\n",
      "Epoch [9/10], Step [14750/15000], Loss: 0.0374\n",
      "Epoch [9/10], Step [15000/15000], Loss: 0.0229\n",
      "Epoch [9/10], Average Loss: 0.0000\n",
      "Epoch [10/10], Step [250/15000], Loss: 0.0190\n",
      "Epoch [10/10], Step [500/15000], Loss: 0.0262\n",
      "Epoch [10/10], Step [750/15000], Loss: 0.0298\n",
      "Epoch [10/10], Step [1000/15000], Loss: 0.0198\n",
      "Epoch [10/10], Step [1250/15000], Loss: 0.0443\n",
      "Epoch [10/10], Step [1500/15000], Loss: 0.0184\n",
      "Epoch [10/10], Step [1750/15000], Loss: 0.0206\n",
      "Epoch [10/10], Step [2000/15000], Loss: 0.0226\n",
      "Epoch [10/10], Step [2250/15000], Loss: 0.0175\n",
      "Epoch [10/10], Step [2500/15000], Loss: 0.0361\n",
      "Epoch [10/10], Step [2750/15000], Loss: 0.0407\n",
      "Epoch [10/10], Step [3000/15000], Loss: 0.0188\n",
      "Epoch [10/10], Step [3250/15000], Loss: 0.0382\n",
      "Epoch [10/10], Step [3500/15000], Loss: 0.0190\n",
      "Epoch [10/10], Step [3750/15000], Loss: 0.0357\n",
      "Epoch [10/10], Step [4000/15000], Loss: 0.0387\n",
      "Epoch [10/10], Step [4250/15000], Loss: 0.0293\n",
      "Epoch [10/10], Step [4500/15000], Loss: 0.0630\n",
      "Epoch [10/10], Step [4750/15000], Loss: 0.0207\n",
      "Epoch [10/10], Step [5000/15000], Loss: 0.0186\n",
      "Epoch [10/10], Step [5250/15000], Loss: 0.0311\n",
      "Epoch [10/10], Step [5500/15000], Loss: 0.0386\n",
      "Epoch [10/10], Step [5750/15000], Loss: 0.0324\n",
      "Epoch [10/10], Step [6000/15000], Loss: 0.0306\n",
      "Epoch [10/10], Step [6250/15000], Loss: 0.0312\n",
      "Epoch [10/10], Step [6500/15000], Loss: 0.0282\n",
      "Epoch [10/10], Step [6750/15000], Loss: 0.0192\n",
      "Epoch [10/10], Step [7000/15000], Loss: 0.0167\n",
      "Epoch [10/10], Step [7250/15000], Loss: 0.0309\n",
      "Epoch [10/10], Step [7500/15000], Loss: 0.0358\n",
      "Epoch [10/10], Step [7750/15000], Loss: 0.0255\n",
      "Epoch [10/10], Step [8000/15000], Loss: 0.0293\n",
      "Epoch [10/10], Step [8250/15000], Loss: 0.0199\n",
      "Epoch [10/10], Step [8500/15000], Loss: 0.0398\n",
      "Epoch [10/10], Step [8750/15000], Loss: 0.0476\n",
      "Epoch [10/10], Step [9000/15000], Loss: 0.0208\n",
      "Epoch [10/10], Step [9250/15000], Loss: 0.0568\n",
      "Epoch [10/10], Step [9500/15000], Loss: 0.0288\n",
      "Epoch [10/10], Step [9750/15000], Loss: 0.0293\n",
      "Epoch [10/10], Step [10000/15000], Loss: 0.0363\n",
      "Epoch [10/10], Step [10250/15000], Loss: 0.0388\n",
      "Epoch [10/10], Step [10500/15000], Loss: 0.0306\n",
      "Epoch [10/10], Step [10750/15000], Loss: 0.0424\n",
      "Epoch [10/10], Step [11000/15000], Loss: 0.0298\n",
      "Epoch [10/10], Step [11250/15000], Loss: 0.0157\n",
      "Epoch [10/10], Step [11500/15000], Loss: 0.0230\n",
      "Epoch [10/10], Step [11750/15000], Loss: 0.0228\n",
      "Epoch [10/10], Step [12000/15000], Loss: 0.0341\n",
      "Epoch [10/10], Step [12250/15000], Loss: 0.0326\n",
      "Epoch [10/10], Step [12500/15000], Loss: 0.0302\n",
      "Epoch [10/10], Step [12750/15000], Loss: 0.0294\n",
      "Epoch [10/10], Step [13000/15000], Loss: 0.0323\n",
      "Epoch [10/10], Step [13250/15000], Loss: 0.0371\n",
      "Epoch [10/10], Step [13500/15000], Loss: 0.0206\n",
      "Epoch [10/10], Step [13750/15000], Loss: 0.0252\n",
      "Epoch [10/10], Step [14000/15000], Loss: 0.0309\n",
      "Epoch [10/10], Step [14250/15000], Loss: 0.0275\n",
      "Epoch [10/10], Step [14500/15000], Loss: 0.0269\n",
      "Epoch [10/10], Step [14750/15000], Loss: 0.0388\n",
      "Epoch [10/10], Step [15000/15000], Loss: 0.0230\n",
      "Epoch [10/10], Average Loss: 0.0000\n",
      "Finished Training\n",
      "Plotting Training Loss over Epochs...\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Model with Learning Rate 0.0001...\n",
      "Starting Testing...\n",
      "Accuracy of the model on the 10000 test images: 97.33%\n",
      "Refining, Training, and Testing Process Completed!\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def refine_train_test_model(model, initial_lr, trainloader, testloader, criterion, num_epochs=10):\n",
    "    \"\"\"\n",
    "    Refines, trains, tests, and plots loss for a given model, learning rate, trainloader, testloader, and loss criterion.\n",
    "    \n",
    "    Parameters:\n",
    "    model: The model to be trained and tested.\n",
    "    initial_lr: The initial learning rate to be adjusted and used for training the model.\n",
    "    trainloader: The DataLoader used for training the model.\n",
    "    testloader: The DataLoader used for testing the model.\n",
    "    criterion: The loss criterion used for training.\n",
    "    num_epochs: The number of epochs for training the model. Default is 10.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Starting refining, training, and testing process...\")\n",
    "    \n",
    "    # Adjust Learning Rate and Initialize Optimizer\n",
    "    print(f\"Adjusting Learning Rate to {initial_lr}...\")\n",
    "    optimizer = optim.Adam(model.parameters(), lr=initial_lr)\n",
    "    \n",
    "    # Train Model with New Learning Rate\n",
    "    print(f\"Training Model with Learning Rate {initial_lr} for {num_epochs} epochs...\")\n",
    "    losses = train_model(model, trainloader, optimizer, criterion, num_epochs)\n",
    "    \n",
    "    # Plot Training Loss over Epochs\n",
    "    print(\"Plotting Training Loss over Epochs...\")\n",
    "    plt.plot(range(num_epochs), losses)\n",
    "    plt.title(f'Training Loss over {num_epochs} Epochs with Learning Rate {initial_lr}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "    \n",
    "    # Test the Model Again\n",
    "    print(f\"Testing Model with Learning Rate {initial_lr}...\")\n",
    "    test_model(model, testloader)\n",
    "    \n",
    "    print(\"Refining, Training, and Testing Process Completed!\")\n",
    "\n",
    "refine_train_test_model(model, 0.0001, trainloader, testloader, criterion, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving your model\n",
    "Using `torch.save`, save your model for future loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to my_mnist_model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'my_mnist_model.pth')\n",
    "print('Model saved to my_mnist_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
